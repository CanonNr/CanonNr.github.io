[{"title":"AQS 首节点为什么为null的虚节点","url":"/2022/01/06/AQS-%E9%A6%96%E8%8A%82%E7%82%B9%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%BAnull%E7%9A%84%E8%99%9A%E8%8A%82%E7%82%B9/","content":"一句话总结节点入队不是原子操作！！！\n等待队列正在有线程进行初始化，但只是进行到了Tail指向Head，没有将Head指向Tail，此时队列中有元素，需要返回True。如果Head没有指向Tail，这种情况下也需要将相关线程加入队列中。所以这块代码是为了解决极端情况下的并发问题。\n具体代码一切要先从 tryAcquire开始protected final boolean tryAcquire(int acquires) &#123;    final Thread current = Thread.currentThread();    int c = getState();    if (c == 0) &#123;        if (!hasQueuedPredecessors() &amp;&amp;            compareAndSetState(0, acquires)) &#123;            setExclusiveOwnerThread(current);            return true;        &#125;    &#125;    ......&#125;\n\n在尝试获取锁的时候如果 State=0(没有线程持有),会将锁的持有者设置为当前线程。\n考虑到并发的问题，并没有因为状态为0就直接 setExclusiveOwnerThread(current)，而是加了一个 hasQueuedPredecessors的判断\ns = h.nextpublic final boolean hasQueuedPredecessors() &#123;\t// The correctness of this depends on head being initialized\t// before tail and on head.next being accurate if the current\t// thread is first in queue.\tNode t = tail; // Read fields in reverse initialization order\tNode h = head;\tNode s;\treturn h != t &amp;&amp; ((s = h.next) == null || s.thread != Thread.currentThread());&#125;// 如果在当前线程之前有一个排队的线程，则为true;// 如果当前线程位于队列的头部或队列为空，则为false\n\n如上是查询是否有线程等待获取的时间超过当前线程。\n如果说此时没有人获取锁，正常来说头和尾都是待初始化阶段都为null，那什么情况下才有可能 h != t？\nh != tprivate Node addWaiter(Node mode) &#123;    Node node = new Node(Thread.currentThread(), mode);    // Try the fast path of enq; backup to full enq on failure    Node pred = tail;    if (pred != null) &#123;        node.prev = pred;//这里只是执行一个快速操作，它和enq里的else分支的逻辑一样        if (compareAndSetTail(pred, node)) &#123;            pred.next = node;            return node;        &#125;    &#125;    enq(node);//如果上面快速操作没有成功，再执行enq    return node;&#125;private Node enq(final Node node) &#123;    for (;;) &#123;//使用for循环，保证入队成功        Node t = tail;        if (t == null) &#123; // 第一次入队，没有dummy node的存在，需先创建它            if (compareAndSetHead(new Node()))                tail = head;        &#125; else &#123; // 至少有一个node，尝试入队            node.prev = t;            if (compareAndSetTail(t, node)) &#123;                t.next = node;                return t;            &#125;        &#125;    &#125;&#125;\n\n在添加新的节点时，如果 tail == null会执行 enq方法，通过死循环保证入列的成功。\ntail == null 表示节点还没有初始化需要创建。\n先通过 CAS方式将 Head设置为 new Node()，设置成功再设置尾指针。\n这个操作肯定不是原子性的，无法保证不被打断。所以 h != t的情况就是发生在 compareAndSetHead(new Node())和 tail = head的间隙中。\n","categories":["Java"],"tags":["多线程与高并发系列","AQS","锁"]},{"title":"AQS基础理解","url":"/2022/01/06/AQS%E5%9F%BA%E7%A1%80%E7%90%86%E8%A7%A3/","content":"前言Java中的大部分同步类（Lock、Semaphore、ReentrantLock等）都是基于 AbstractQueuedSynchronizer （简称为AQS）实现的，中文名：抽象队列同步器。\nAQS是一种提供了原子式管理同步状态、阻塞和唤醒线程功能以及队列模型的简单框架。\n核心思想\n如果被请求的共享资源空闲，那么就将当前请求资源的线程设置为有效的工作线程，将共享资源设置为锁定状态；\n如果共享资源被占用，就需要一定的阻塞等待唤醒机制来保证锁分配。\n这个机制主要用的是CLH队列的变体实现的，将暂时获取不到锁的线程加入到队列中。\n\n\nCLH：Craig、Landin and Hagersten队列，是单向链表，AQS中的队列是CLH变体的虚拟双向队列（FIFO），AQS是通过将每条请求共享资源的线程封装成一个节点来实现锁的分配。\nAQS使用一个 Volatile 的 int 类型的成员变量来表示同步状态，通过内置的FIFO队列来完成资源获取的排队工作，通过CAS完成对State值的修改。\n\n\n当第一个线程进来查询 state 的状态，如果  state = 0  则获取锁，如果 大于 0 则表示已经被获取，需要进入CLH队列等待。\n\n所以AQS的本质是\n一个Volatile 修饰 int 类型的 State\nCAS\n双向链表\n\n关于性能AQS通过CAS操作代替了 synchronized 的重量级锁，性能得到了提升，但是在JDk1.5之后 synchronized带来了锁升级的机制，二者性能上差距已经不大，针对不同的场景进行不同的选型\n","categories":["Java"],"tags":["多线程与高并发系列","AQS","锁"]},{"title":"First Test Post","url":"/2022/01/06/First-Test-Post/","content":"Test这是一个测试的文章一个图片的测试http\nhttps\n"},{"title":"JVM 内存结构","url":"/2022/01/08/JVM-%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84/","content":"前言根据上图我们发现 Java 内存结构由堆、虚拟机栈、方法区、程序计算器、本地方法栈。如果分为两种类型则是：线程共享的 和 线程私有的。\n线程私有程序计数器（PC Register）在图中看到了很大一块程序计数器其实在现实中只是开辟了很小很小的一部分，他的核心作用是用于：记录线程执行到的位置。\n\n由于Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间的计数器互不影响，独立存储。\n\n\n程序计数器中只存储当前线程执行程序的行号，一个类指针的数据结构。JVM线程中执行的方法有2种类型：普通Java方法和由其他语言实现的native方法。如果当前执行的是普通Java方法，则程序计数器记录的是虚拟机字节码指令的地址。如果当前执行的是native方法，则计数器的值为空（Undefined）。\n\n虚拟机栈（VM Stack）\n和程序计数器一样，虚拟机栈也是线程私有的，即生命周期和线程相同。\nJava虚拟机栈和线程同时创建，用于存储栈帧。每个方法在执行时都会创建一个栈帧(Stack Frame)，用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用直到执行完成的过程就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。栈帧：一个栈帧随着一个方法的调用开始而创建，这个方法调用完成而销毁。栈帧内存放者方法中的局部变量，操作数栈等数据。栈帧也叫过程活动记录，是编译器用来实现过程/函数调用的一种数据结构。\n\n\n\n栈帧结构\n\n局部变量表局部变量表（Local Variable Table）是一组变量值存储空间，用于存放方法参数和方法内部定义的局部变量。并且在Java编译为Class文件时，就已经确定了该方法所需要分配的局部变量表的最大容量。\n\n动态连接每个栈帧都包含一个指向运行时常量池中该栈帧所属方法的引用，持有这个引用是为了支付方法调用过程中的动态连接Dynamic Linking）。\n\n操作数据栈操作数栈和局部变量表一样，在编译时期就已经确定了该方法所需要分配的局部变量表的最大容量。\n\n返回方法当一个方法开始执行后，只有2种方式可以退出这个方法 ：\n\n方法返回指令 执行引擎遇到一个方法返回的字节码指令，这时候有可能会有返回值传递给上层的方法调用者，这种退出方式称为正常完成出口。\n异常退出 ： 在方法执行过程中遇到了异常，并且没有处理这个异常，就会导致方法退出。\n\n\n\n\n\n本地方法栈（Native Method Stack）简单来说就是一个java调用非java代码的接口，一般是C或C++代码。至于为什么使用其他语言主要是使用更底层的语言效率更高。\n线程共享堆（Heap）\nJAVA堆内存管理是影响性能主要因素之一也是虚拟机管理中心内存最大的一块，在虚拟机启时创建。此内存区域唯一的目的就是存放对象实例，Java中几乎所有的对象实例都在这里分配内存。\n\n\n下面通过一个图片看一下组成部分：\n新生代\n\n\n新生成的对象首先放到年轻代Eden区，当Eden空间满了，触发Minor GC，存活下来的对象移动到Survivor0区，Survivor0区满后触发执行Minor GC，Survivor0区存活对象移动到Suvivor1区，这样保证了一段时间内总有一个survivor区为空。经过多次Minor GC仍然存活的对象移动到老年代。\n\n结构组成：\n\n伊甸园区\n幸存者一区\n幸存者二区\n\n\n💡 新生代特点：每次垃圾回收时都有大量的对象需要被回收。垃圾回收器在新生代采用的收集算法是Copying(复制)方法：它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块面对内存用完了，就将还存活的对象复制到另外一块，然后再把已使用的内存空间一次清理掉。由于新生代有每次垃圾回收时都有大量的对象需要被回收的特点，所以采用Copying算法，复制的存活对象较少，性能比较好。但是Copying算法还是有缺陷的，那就是内存的使用率只有一半。\n\n\n内存分配\n堆的大小可以通过参数 –Xms、-Xmx来指定。\n新生代 ( Young ) 与老年代 ( Old ) 的默认比例的值为 1:2 ( 该值可以通过参数 –XX:NewRatio 来指定 )\n伊甸区 : 幸存一区 : 幸存二区 = 8 : 1 : 1 ( 可以通过参数 –XX:SurvivorRatio 来设定 )，\n老年代\n\n\n老年代存储长期存活的对象，占满时会触发Major GC=Full GC，GC期间会停止所有线程等待GC完成，所以对响应要求高的应用尽量减少发生Major GC，避免响应超时。\n\n\n\n永久代永久代存放类的常量、静态变量等信息，是方法区的实现。对永久代的回收主要回收两部分内容：废弃常量和无用的类。\n\n\n💡 注意：JDK1.8 之后，不存在永久带，即没有java.lang.OutOfMemoryError: PermGen space 这种错误。取而代之的是Meta space（元空间）。之前存储在永久带的字符串常量，静态变量移到了堆中去了，元空间不存储在堆中，而是存储在本地内存中。\n\n方法区（Method Area）方法区是被所有线程共享，所有字段和字节码，以及一些特殊方法如构造器，接口代码也在此定义。简单说，所有定义的方法的信息都保存在该区域，此区域属于共享区间。静态变量 + 常量 + 类信息 + 运行时常量池存在方法区中，实例对象存在堆内存中。\n","categories":["Java"],"tags":["JVM","造火箭"]},{"title":"Java new 一个对象都经历了什么","url":"/2022/01/08/Java-new-%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E9%83%BD%E7%BB%8F%E5%8E%86%E4%BA%86%E4%BB%80%E4%B9%88/","content":"前言\nJava在new一个对象的时候，会先查看对象所属的类有没有被加载到内存，如果没有的话，就会先通过类的全限定名来加载。加载并初始化类完成后，再进行对象的创建工作。由此得出一个new一个java对象包括两个过程：加载和创建\n\n加载类\njava是使用双亲委派模型来进行类的加载的，具体查看双亲委派的笔记。\n\n1.加载由类加载器负责根据一个类的全限定类名来读取此类的二进制字节流到JVM内部，并存储在运行时内存区的方法区，然后将其转换为一个与目标类型对应的java.lang.Class对象实例​\n2.验证\n格式验证：验证是否符合class文件规范\n是否以0xCAFEBABE开头\n主、次版本号是否在当前虚拟机的处理范围之内\n\n\n\n3.准备在方法区内为类静态变量分配内存并将其初始化为默认值。被 final 修饰的 static 变量（常量），会直接赋值。\n4.解析解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。\n\n符号引用：字符串类型的引用，能根据这个字符串定位到指定的对象，如：java/lang/StringBuilder\n\n直接引用：内存地址\n5.初始化\n初始化是指为类静态变量赋予正确的初始值，JVM负责对类进行初始化，主要对类变量进行初始化。在Java中对类变量进行初始值设定有两种方式：\n\n声明类变量时指定初始值；\n使用静态代码块为类变量指定初始值。\n\n\n执行static代码块\n\nstatic 代码块只有jvm能够调用;如果 static 代码块有多个，JVM将按照它们在类中出现的先后顺序依次执行它们，每个代码块只会被执行一次。**\n\n\n\n创建对象1.在堆区分配对象需要的内存分配的内存包括本类和父类的所有实例变量，但不包括任何静态变量\n2.对所有实例变量赋默认值将方法区内对实例变量的定义拷贝一份到堆区，然后赋默认值\n3.执行实例初始化代码初始化顺序是先初始化父类再初始化子类，初始化时先执行实例代码块然后是构造方法\n4.定义栈变量如果new的对象会赋值给一个变量，则会在栈区内定义一个该类型的引用变量，然后将堆区的对象地址赋予它。\n","categories":["Java"],"tags":["JVM"]},{"title":"Redis数据结构源码解析(〇) 五大数据类型","url":"/2022/01/18/Redis%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E3%80%87-%E4%BA%94%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","content":"前言本文不介绍具体使用，已底层实现为主。先宏观介绍下实现方式具体实现后续文章介绍。\n数据类型\n字符串 String\n列表 List\n集合 Set\n有序集合 ZSet\n哈希 Hash\n\n数据编码#define OBJ_ENCODING_RAW 0     /* Raw representation */#define OBJ_ENCODING_INT 1     /* Encoded as integer */#define OBJ_ENCODING_HT 2      /* Encoded as hash table */#define OBJ_ENCODING_ZIPMAP 3  /* Encoded as zipmap */#define OBJ_ENCODING_LINKEDLIST 4 /* No longer used: old list encoding. */#define OBJ_ENCODING_ZIPLIST 5 /* Encoded as ziplist */#define OBJ_ENCODING_INTSET 6  /* Encoded as intset */#define OBJ_ENCODING_SKIPLIST 7  /* Encoded as skiplist */#define OBJ_ENCODING_EMBSTR 8  /* Embedded sds string encoding */#define OBJ_ENCODING_QUICKLIST 9 /* Encoded as linked list of ziplists */#define OBJ_ENCODING_STREAM 10 /* Encoded as a radix tree of listpacks */\n\n底层实现\n一 、字符串使用了三种作为底层的实现：\n\nembstr（OBJ_ENCODING_EMBSTR）\n长度小于44字节的字符串，该编码中 RedisObject、SDS放置在连续的内存中。\n\nrow（OBJ_ENCODING_RAW）\n长度大于44字节,除此之外与 OBJ_ENCODING_EMBSTR区别仅在于是否为连续内存\n\nint（OBJ_ENCODING_INT）\n有时候我们会在String寸INT类型的自然数，例如一个手机号如果为字符串则需要11字节，如果使用long long 只需要8字节。\n\n\n3中的 Int没什么好说的属于基本数据类型，1和2都涉及到SDS，这也是Redis中很基础的一个对象。\n二 、列表使用了三种作为底层的实现：\n\n双向链表（OBJ_ENCODING_LINKEDLIST）\n链表节点指针比较多，内存浪费。\n链表会造成内存碎片\n\n\n压缩列表（OBJ_ENCODING_ZIPLIST）\n底层为数组实现，解决了内存碎片问题，不需要指针\n当元素很多时，可用内存明明足够，但是因为没有足够大的连续内存而出问题\n数组的新增、删除牵一发动全身，需要整个列表进行数据操作\n\n\n快速列表（OBJ_ENCODING_ZIPLIST）\n基于压缩列表和链表实现，解决了压缩列表存在的一些问题，通过分治思想把很大的列表拆分为多个\n\n\n\n3.2版本之后弃用了双向链表改为快速列表。\n那么什么时候使用压缩列表什么时候使用快速列表？\n\n所有字符串元素的长度都小于 64 字节并且保存的元素数量小于512个的列表使用压缩列表\n存在字符串元素长度大于64或元素个数大于512个使用快速列表\n\n三、Hash使用了两种作为底层的实现：\n\n字典（哈希表）(OBJ_ENCODING_HT)\n压缩列表（OBJ_ENCODING_ZIPLIST）\n\n关于两者的选择其实比较简单：\n\n因为压缩列表比字典更节省内存， 所以程序在创建新 Hash 键时， 默认使用压缩列表作为底层实现， 当有需要时， 程序才会将底层实现从压缩列表转换到字典。\n\n\n总长度超过512字节或者单个元素长度大于64使用字典\n总长度小于512字节或者单个元素长度小于64使用压缩列表\n\n四、Set使用了两种作为底层的实现：\n\n整数集合 (OBJ_ENCODING_INTSET)\n字典（哈希表）(OBJ_ENCODING_HT)\n\n关于两者的选择：\n\nRedis没有规定Set可存放的数据类型为什么，可能遇到与String同样的问题。存储一个手机号，使用字符串和long类型内存的占用是完全不一样的。\n真的是能省就省\n\n\n集合对象保存的所有元素都是整数值且元素数量不超过 512 个使用整数集合\n总长度超过512字节或者单个元素长度大于64的使用字典\n遇到如下情况会立刻发生转换\n往集合中添加了非整型变量会转换为字典\n\n\n\n五、ZSet使用了两种作为底层的实现：\n\n跳跃表（OBJ_ENCODING_SKIPLIST）\n压缩列表（OBJ_ENCODING_ZIPLIST）\n\n两者的选择也在于内存的使用上：\n\n在大量元素的集合中使用跳跃表可以更快的查询，但是如果数量不多还要存储大量的多层索引会造成内存的浪费。\n\n\nzset会根据 zadd命令添加的第一个元素的长度大小来选择编码方式：满足 zset_max_ziplist_entries的值不为0，第一个元素的长度小于 server.zset_max_ziplist_value，使用压缩列表，否则就是跳跃表。\n待新加的新的字符串长度超过 zset_max_ziplist_value（默认值64）时或者 ziplist保存的节点数量超过 server.zset_max_ziplist_entries（默认值128）时使用skiplist。\n\n总结\n\n\n数据类型\nString\nList\nHash\nSet\nZSet\n\n\n\nSDS\n√\n\n\n\n\n\n\n快速列表\n\n√\n\n\n\n\n\n双向链表\n\n√\n\n\n\n\n\n压缩列表\n\n√\n√\n\n√\n\n\n跳跃表\n\n\n\n\n√\n\n\n字典（哈希表）\n\n\n√\n√\n\n\n\n整数集合\n\n\n\n√\n\n\n\n致谢\nRedis源码解析 - 有赞技术团队\nRedis 设计与实现\n\n","categories":["Redis"],"tags":["源码","Redis数据结构源码解析系列"]},{"title":"Redis数据结构源码解析(一) SDS","url":"/2022/01/18/Redis%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E4%B8%80-SDS/","content":"C语言存储字符串的问题C语言中没有类似Java中的String高级对象在Java中 String用的不亦乐乎，其自带的 equals、split、charAt等方法真的很方便，但是C是没有这种高级对象的。\n如果在C语言中使用字符串有两种方式：\n\n使用字符串指针 const char *str = &quot;hello&quot;;\n使用数组 const char str[] = &quot;hello&quot;;\n\n方法一最大的缺陷在于字符串定义后即不能修改，所以更多情况下还是使用数组。\n二进制安全C语言中表示字符串结尾的符号是 \\0 ，如果字符串本身就具有 \\0 字符，就会被截断，即非二进制安全。\n计算字符串的长度性能低C语言中有一个计算字符串长度的函数 strlen，但这个函数与Java的不一样，需要遍历整个字符串来计算长度，时间复杂度是O（n），如果需要在循环中计算，性能将十分低下。\n字符串拼接性能低因为C语言字符串不记录长度，对于一个长度n的字符串来说，底层是n+1的字符数组（n是我们要存的字符，1是最后的结束符 \\0）。\n\n底层虽然使用了n+1的字符数组，但是在使用 strlen方法的时候不会计算 \\0\n\nSimple Dynamic String 简单的动态字符串在 redis3.2之前，sds只有一个类型，定义如下：\nstruct sdshdr &#123;    unsigned int len; // 当前的长度    unsigned int free; // 剩余可用长度    char buf[]; // 实际存放的地方&#125;;\n\n在后续版本中改为了\nstruct __attribute__ ((__packed__)) sdshdr5 &#123;    unsigned char flags; /* 3 lsb of type, and 5 msb of string length */    char buf[];&#125;;struct __attribute__ ((__packed__)) sdshdr8 &#123;    uint8_t len; // sds的当前长度(单位是字节)    uint8_t alloc; // sds分配的内存大小(单位是字节)    unsigned char flags; // sdshdr的类型    char buf[]; // 实际存放的地方&#125;;struct __attribute__ ((__packed__)) sdshdr16 &#123;    uint16_t len;     uint16_t alloc;     unsigned char flags;     char buf[];&#125;;....\n\nsdshdr5、sdshdr8、sdshdr16、sdshdr32、sdshdr64 一共五个方法，其中官方注解有些到 sdshdr5还没有被使用过。\n类型机制上面介绍了一共有五个 sdshdrX方法，看着都差不多只是 len和 alloc 类型不同，为什么要这么做呢？\n其实也很简单就是省内存，而且真的是把能省就省发挥到了机制，在做业务开发的时候很少会为了这点内存再去做一套特殊的处理…\n节省内存我们已知的是 len和 alloc服务于字符串的长度，分别记录字符串长度和\n\nuint8: 字符串的长度最大(2^8)-1  = 255\nuint16: 字符串的长度最大(2^16)-1 = 35535\nuint32: 字符串的长度最大(2^32)-1 = 4294967295\nuint64: 字符串的长度最大(2^64)-1 = 1.8446744e+19\n\n绝大多数情况下字符串的长度都不会超过35536，如果使用 uint64存储几十个几百就太浪费了。\n最大长度问题虽然类型限制了最大长度为(2^64)-1 ，但是真正限制的并不是它。\ngetBitOffsetFromArgument 是一个计算偏移量防止为负或者溢出的方法。在其方法内有这么一个判断：\n/* Limit offset to server.proto_max_bulk_len (512MB in bytes by default) */if ((loffset &lt; 0) || (loffset &gt;&gt; 3) &gt;= server.proto_max_bulk_len)&#123;    addReplyError(c,err);    return C_ERR;&#125;\n\n根据源码注释可以看到，他是设置了512MB的一个长度限制，超出会返回异常。\n在 config.c 文件中会对配置对象统一的定义。\ncreateLongLongConfig(&quot;proto-max-bulk-len&quot;, NULL, MODIFIABLE_CONFIG, 1024*1024, LONG_MAX, server.proto_max_bulk_len, 512ll*1024*1024, MEMORY_CONFIG, NULL, NULL), /* Bulk request max size */\n\n字符串的追加与扩容redis&gt; SET msg &quot;hello world&quot;OKredis&gt; APPEND msg &quot; again!&quot;(integer) 18redis&gt; GET msg&quot;hello world again!&quot;\n\n如上执行了一个简单的字符串追加工作。接下来以伪代码的形式介绍整个过程经历了什么。\nSET 命令，创建一个SDS对象struct sdshdr &#123;    len = 11;    free = 0;    buf = &quot;hello world\\0&quot;;&#125;\n\n执行 APPEND 命令此时相应的 sdshdr 被更新，字符串 &quot; again!&quot; 会被追加到原来的 &quot;hello world&quot; 之后：\nstruct sdshdr &#123;    len = 18;    free = 18;    buf = &quot;hello world again!\\0                  &quot;;     // 空白的地方为预分配空间，共 18 + 18 + 1 个字节&#125;\n\n不难发现，free从0-&gt;18，len从11-&gt;18，这次扩容整体从11-&gt;22（18-11+18）。\n具体代码\nsds sdscatlen(sds s, const void *t, size_t len) &#123;    size_t curlen = sdslen(s);      // 获取当前字符串的长度    s = sdsMakeRoomFor(s,len);      // 扩容    if (s == NULL) return NULL;    memcpy(s+curlen, t, len);       // 内存拷贝    sdssetlen(s, curlen+len);       // 更新len属性    s[curlen+len] = &#x27;\\0&#x27;;           // 末尾追加一个\\0    return s;&#125;\n\nsds sdsMakeRoomFor(sds s, size_t addlen) &#123;    void *sh, *newsh;  //定义两个 sdshdr 结构体指针    size_t avail = sdsavail(s); // 获取 s 目前空闲空间长度    size_t len, newlen, reqlen; // len为扩展前长度,newlen为扩展后长度    char type, oldtype = s[-1] &amp; SDS_TYPE_MASK; // 用作sdsType的判断    int hdrlen;    size_t usable;    /* Return ASAP if there is enough space left. */     if (avail &gt;= addlen) return s; // 如果可用空间大于添加的长度，直接返回    len = sdslen(s);// 获取 s 目前已占用空间的长度    sh = (char*)s-sdsHdrSize(oldtype);  //结构体指针赋值    reqlen = newlen = (len+addlen); // 复制扩展后长度,原长度+扩展的长度数    assert(newlen &gt; len);   /* Catch size_t overflow */    // 开始设定扩展后长度    if (newlen &lt; SDS_MAX_PREALLOC)        // 如果新长度小于SDS_MAX_PREALLOC(1M),新长度直接翻倍        // 这也就是上面为什么从11-&gt;22        newlen *= 2;    else        // 否则每次新增一个SDS_MAX_PREALLOC单位也就是1M        newlen += SDS_MAX_PREALLOC;    // 此时有了新长度需要重新获取一下sds的类型,也就是sdshdr5,sdshdr8...    type = sdsReqType(newlen);    /* Don&#x27;t use type 5: the user is appending to the string and type 5 is     * not able to remember empty space, so sdsMakeRoomFor() must be called     * at every appending operation. */    // 作者示意不要使用sdshdr5,会变转换为sdshdr8    if (type == SDS_TYPE_5) type = SDS_TYPE_8;    hdrlen = sdsHdrSize(type);    assert(hdrlen + newlen + 1 &gt; reqlen);  /* Catch size_t overflow */    // 对比扩容前后类型是否改变，做对应的处理，不重要    if (oldtype==type) &#123;        // 没有改变,在原空间分配        newsh = s_realloc_usable(sh, hdrlen+newlen+1, &amp;usable);        if (newsh == NULL) return NULL;        s = (char*)newsh+hdrlen;    &#125; else &#123;        /* Since the header size changes, need to move the string forward,         * and can&#x27;t use realloc */        // 重新分配空间        // 并删除原空间        newsh = s_malloc_usable(hdrlen+newlen+1, &amp;usable);        if (newsh == NULL) return NULL;        memcpy((char*)newsh+hdrlen, s, len+1);        s_free(sh);        s = (char*)newsh+hdrlen;        s[-1] = type;        sdssetlen(s, len);    &#125;    usable = usable-hdrlen-1;    if (usable &gt; sdsTypeMaxSize(type))        usable = sdsTypeMaxSize(type);    sdssetalloc(s, usable);    return s;&#125;\n\n编码格式在Redis中为了进一步的节省内存还为SDS设置了三种编码格式：\n\nOBJ_ENCODING_EMBSTR：长度小于44字节的字符串，该编码中 RedisObject、SDS放置在连续的内存中。\nOBJ_ENCODING_RAW：长度大于44字节,除此之外与 OBJ_ENCODING_EMBSTR区别仅在于是否为连续内存\nOBJ_ENCODING_INT: 有时候我们会在String寸INT类型的自然数，例如一个手机号如果为字符串则需要11字节，如果使用long long 只需要8字节。\n\n总结\nRedis 的字符串表示为 sds ，而不是 C 字符串（以 \\0 结尾的 char*），相比较有如下特性\n在计算字符长度时更高效，直接读取 len熟悉为O(1)复杂度，传统C语言需要逐个遍历为O(n)\n提供更高效的扩容机制，牺牲一部分内存的情况下换取更快的追加速度\n二进制安全，不强制通过 \\0作为结尾。也就是说在SDS中可以存储任何值\n\n\n\n致谢\nRedis源码解析 - 有赞技术团队\nRedis 设计与实现\nRedis动态字符串SDS源码学习\n\n","categories":["Redis"],"tags":["源码","Redis数据结构源码解析系列"]},{"title":"Redis数据结构源码解析(七) 整数集合","url":"/2022/01/20/Redis%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E4%B8%83-%E6%95%B4%E6%95%B0%E9%9B%86%E5%90%88/","content":"概念整数集合（intset）是一个有序的、存储整型数据的结构。\n关键字：有序的、整型的\n\n\nconding决定了的element的长度，对应关系如下\n\n\n基本结构intsettypedef struct intset &#123;      //编码    uint32_t encoding;    //元素个数    uint32_t length;    // 柔性数组，根据encoding 决定几个字节表示一个数组    int8_t contents[];&#125; intset;\n\n可以看到底层是通过数组实现\n基本操作查询uint8_t intsetFind(intset *is, int64_t value) &#123;  　　uint8_t valenc = _intsetValueEncoding(value); //判断编码方式　　//编码方式如果大于当前intset的编码方式，直接返回0。否则调用intsetSearch函数进行查找　　return valenc &lt;= intrev32ifbe(is-&gt;encoding) &amp;&amp; intsetSearch(is,value,NULL);&#125;static uint8_t intsetSearch(intset *is, int64_t value, uint32_t *pos) &#123;      int min = 0, max = intrev32ifbe(is-&gt;length)-1, mid = -1;      int64_t cur = -1;      /*如果intset中没有元素，直接返回0 */    if (intrev32ifbe(is-&gt;length) == 0) &#123;        if (pos) *pos = 0;      return 0;    &#125; else &#123;    /* 如果元素大于最大值或者小于最小值，直接返回0 */    if (value &gt; _intsetGet(is,max)) &#123;        if (pos) *pos = intrev32ifbe(is-&gt;length);        return 0;    &#125; else if (value &lt; _intsetGet(is,0)) &#123;      if (pos) *pos = 0;      return 0;        &#125;    &#125;    while(max &gt;= min) &#123; //二分查找该元素        mid = ((unsigned int)min + (unsigned int)max) &gt;&gt; 1;      cur = _intsetGet(is,mid);    if (value &gt; cur) &#123;        min = mid+1;    &#125; else if (value &lt; cur) &#123;      max = mid-1;    &#125; else &#123;      break;        &#125;    &#125;    if (value == cur) &#123; //查找到返回1，未查找到返回0        if (pos) *pos = mid;      return 1;    &#125; else &#123;      if (pos) *pos = min;      return 0;     &#125;  &#125;&#125;\n\n\n其实比较简单，就是先做了一些基础的结构判断，因为是有序的可以通过二分查找快速查询。\n\n总结这应该是最简单的一个类型了，就是一个基于数组的有序集合，增删改查都使用到了二分查找。\n致谢\nRedis源码解析 - 有赞技术团队\n\n","categories":["Redis"],"tags":["源码","Redis数据结构源码解析系列"]},{"title":"Redis数据结构源码解析(三) 跳跃表","url":"/2022/01/19/Redis%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E4%B8%89-%E8%B7%B3%E8%B7%83%E8%A1%A8/","content":"概念跳跃表类似一个多层的链表，首先从最高层开始查找，如果下一个节点的值大于要查找的值或者下一个节点为null,则往下一层查找。通过空间换时间的策略，将时间复杂度控制在O(logn)。\n图例\n例如查找51这个数\n首先从第一层开始查找，找到第二个节点，发现后面为null。\n从第二层查找 查找到第四个节点，发现后面的节点为61，大于当前的数。\n从第三层查找 查找到第六个节点 结束 一共查找四次，比遍历一次少了两次。数据量大的情况下，这个性能会提升的很明显。\n核心概念\n表头：负责维护跳跃表的节点指针。\n跳跃表节点：保存着元素值，以及多个层。\n层：保存着指向其他元素的指针。高层的指针越过的元素数量大于等于低层的指针，为了提高查找的效率，程序总是从高层先开始访问，然后随着元素值范围的缩小，慢慢降低层次。\n表尾：全部由 NULL 组成，表示跳跃表的末尾。\n\n基本结构zskiplistNodetypedef struct zskiplistNode &#123;    // 内容    sds ele;    // 分值    double score;    // 后退指针，指向当前节点底层 前一个节点    struct zskiplistNode *backward;    struct zskiplistLevel &#123;        struct zskiplistNode *forward; // 指向当前层的前一个节点        unsigned long span;\t\t// forward 指向前一个节点的与当前节点的间距    &#125; level[]; // level是一个数组&#125; zskiplistNode;\n\nzskiplisttypedef struct zskiplist &#123;      struct zskiplistNode *header, *tail; // 分别指向头结点和尾结点    unsigned long length; // 跳跃表总长度    int level; // 跳跃表总高度&#125; zskiplist;\n\n\n其中，头节点是跳跃表的一个特殊节点，它的level数组元素个数为64。头节点在有序集合中不存储任何member和score值，ele值为NULL, score值为0；也不计入跳跃表的总长度。头节点在初始化时，64个元素的forward都指向NULL, span值都为0。\n\n创建跳跃表zslCreatezskiplist *zslCreate(void) &#123;    int j;    zskiplist *zsl;    zsl = zmalloc(sizeof(*zsl)); // 申请内存    zsl-&gt;level = 1; // 初创建 级数为1    zsl-&gt;length = 0; // 初创建 长度为1    zsl-&gt;header = zslCreateNode(ZSKIPLIST_MAXLEVEL,0,NULL); // 创建一个默认的头结点    // 此处循环了32次    for (j = 0; j &lt; ZSKIPLIST_MAXLEVEL; j++) &#123;        zsl-&gt;header-&gt;level[j].forward = NULL;        zsl-&gt;header-&gt;level[j].span = 0;    &#125;    zsl-&gt;header-&gt;backward = NULL;    zsl-&gt;tail = NULL;    return zsl;&#125;\n\n\nZSKIPLIST_MAXLEVEL 跳跃表最大等级(默认值为32)，可以容纳2^64个元素\n为跳跃表创建了一个初始的头节点，并设置了一个32层高的索引\n\n创建表节点zslCreateNode/* Create a skiplist node with the specified number of levels. * The SDS string &#x27;ele&#x27; is referenced by the node after the call. */zskiplistNode *zslCreateNode(int level, double score, sds ele) &#123;    zskiplistNode *zn =        zmalloc(sizeof(*zn)+level*sizeof(struct zskiplistLevel));    zn-&gt;score = score;    zn-&gt;ele = ele;    return zn;&#125;\n\n\n又是先创建节点再赋值\n\n随机层高zslRandomLevelint zslRandomLevel(void) &#123;    int level = 1;    while ((random()&amp;0xFFFF) &lt; (ZSKIPLIST_P * 0xFFFF))        level += 1;    return (level&lt;ZSKIPLIST_MAXLEVEL) ? level : ZSKIPLIST_MAXLEVEL;&#125;\n\n在首节点直接拉满设置了一个层高的最大值32，但是如果每一个元素的层高都是32跳跃表也就没意义了。这个方法就是确定节点的层高，返回一个1 ~ ZSKIPLIST_MAXLEVEL 的数。\n插入节点插入节点总的来说一共四步\n\n查找插入位置\n调整高度\n插入节点\n调整 backward\n\nzslInsertzskiplistNode *zslInsert(zskiplist *zsl, double score, sds ele) &#123;      zskiplistNode *update[ZSKIPLIST_MAXLEVEL], *x;    unsigned int rank[ZSKIPLIST_MAXLEVEL];    int i, level;    serverAssert(!isnan(score));    // 查找节点    x = zsl-&gt;header;    for (i = zsl-&gt;level-1; i &gt;= 0; i--) &#123;        /* store rank that is crossed to reach the insert position */        rank[i] = i == (zsl-&gt;level-1) ? 0 : rank[i+1];        while (x-&gt;level[i].forward &amp;&amp;                (x-&gt;level[i].forward-&gt;score &lt; score ||                    (x-&gt;level[i].forward-&gt;score == score &amp;&amp;                    sdscmp(x-&gt;level[i].forward-&gt;ele,ele) &lt; 0)))        &#123;            rank[i] += x-&gt;level[i].span;            x = x-&gt;level[i].forward;        &#125;        update[i] = x;    &#125;      // 随机一个层数    level = zslRandomLevel();    if (level &gt; zsl-&gt;level) &#123;        for (i = zsl-&gt;level; i &lt; level; i++) &#123;            rank[i] = 0;            update[i] = zsl-&gt;header;            update[i]-&gt;level[i].span = zsl-&gt;length;        &#125;        zsl-&gt;level = level;    &#125;    x = zslCreateNode(level,score,ele);    //插入节点    for (i = 0; i &lt; level; i++) &#123;        x-&gt;level[i].forward = update[i]-&gt;level[i].forward;        update[i]-&gt;level[i].forward = x;        /* update span covered by update[i] as x is inserted here */        x-&gt;level[i].span = update[i]-&gt;level[i].span - (rank[0] - rank[i]);        update[i]-&gt;level[i].span = (rank[0] - rank[i]) + 1;    &#125;    /* increment span for untouched levels */    for (i = level; i &lt; zsl-&gt;level; i++) &#123;        update[i]-&gt;level[i].span++;    &#125;    x-&gt;backward = (update[0] == zsl-&gt;header) ? NULL : update[0];    if (x-&gt;level[0].forward)        x-&gt;level[0].forward-&gt;backward = x;    else        zsl-&gt;tail = x;    zsl-&gt;length++;    return x;&#125;\n\n删除节点\n删除相比较会简单一些，如果插入或者查询时需要先查到删除节点的位置。\n删除节点（其实就是一个链表删除元素）\n是否为唯一的高节点，如果是则更新 level,不是则无需额外处理\n\n其他为什么不使用红黑树引用一下原作者的话\n\nThere are a few reasons: They are not very memory intensive. It’s up to you basically. Changing parameters about the probability of a node to have a given number of levels will make then less memory intensive than btrees.A sorted set is often target of many ZRANGE or ZREVRANGE operations, that is, traversing the skip list as a linked list. With this operation the cache locality of skip lists is at least as good as with other kind of balanced trees.They are simpler to implement, debug, and so forth. For instance thanks to the skip list simplicity I received a patch (already in Redis master) with augmented skip lists implementing ZRANK in O(log(N)). It required little changes to the code.About the Append Only durability &amp; speed, I don’t think it is a good idea to optimize Redis at cost of more code and more complexity for a use case that IMHO should be rare for the Redis target (fsync() at every command). Almost no one is using this feature even with ACID SQL databases, as the performance hint is big anyway.About threads: our experience shows that Redis is mostly I/O bound. I’m using threads to serve things from Virtual Memory. The long term solution to exploit all the cores, assuming your link is so fast that you can saturate a single core, is running multiple instances of Redis (no locks, almost fully scalable linearly with number of cores), and using the “Redis Cluster” solution that I plan to develop in the future.\n\n\n简单总结一下：\n这并不会浪费太多的空间，并且树的高度可以动态调整的。\nZRANGE 和 ZREVRANGE命令，跳表性能比红黑树好\n红黑树比较复杂…作者懒得实现\n\n\n\n总结\nRedis使用跳跃表结构存储有序集合数据,通过概率平衡实现近似平衡p叉书的存取效率。\n\n致谢\nRedis源码解析 - 有赞技术团队\nRedis 设计与实现\n\n","categories":["Redis"],"tags":["源码","Redis数据结构源码解析系列"]},{"title":"Redis数据结构源码解析(二) 字典","url":"/2022/01/19/Redis%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E4%BA%8C-%E5%AD%97%E5%85%B8/","content":"对比HashMap字典的使用和底层与Java中的HashMap还是很像的，比较特殊的就是扩容方式。\n基本结构\ndicttypedef struct dict &#123;    // 操作类型    dictType *type;      // 依赖的数据    void *privdata;\t     // Hash表,是个数组两个元素    dictht ht[2];    // -1代表没有进行rehash值，否则代表hash操作进行到了哪个索引    long rehashidx; /* rehashing not in progress if rehashidx == -1 */    // 当前运行的迭代器数    int16_t pauserehash; /* If &gt;0 rehashing is paused (&lt;0 indicates coding error) */&#125; dict;\n\ndicthttypedef struct dictht &#123;      // 二维数组    dictEntry **table;    // table总大小    unsigned long size;    // 掩码=size-1    unsigned long sizemask;    // 已经保存的键值对    unsigned long used;&#125; dictht;\n\ndictEntrytypedef struct dictEntry &#123;      //键    void *key;    //值    union &#123;        void *val; //值        uint64_t u64;        int64_t s64; //过期时间        double d;    &#125; v;    // hash冲突的next指针    struct dictEntry *next;&#125; dictEntry;\n\n如何应对Hash冲突既然使用了Hash表，就逃不掉Hash冲突的问题。\nJava中的解决方式\nHashMap 使用了数组+链表+红黑树的底层结构，每一个节点对象都会有 next属性，从而形成一个链表结构。当链表长度大于8时会升级为红黑树。\n\nstatic class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123;    final int hash;    final K key;    V value;    Node&lt;K,V&gt; next;    Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123;        this.hash = hash;        this.key = key;        this.value = value;        this.next = next;    &#125;\n\n其实这叫拉链法或者叫链地址法\n拉链法\n把具有相同散列地址的关键字(同义词)值放在同一个单链表中，称为同义词链表。\n\n添加元素dictAdd/* Add an element to the target hash table */int dictAdd(dict *d, void *key, void *val)&#123;    // 新增元素,但是没有设置具体的值    dictEntry *entry = dictAddRaw(d,key,NULL);\t// 新增失败则返回异常    if (!entry) return DICT_ERR;    // 设置真实值    dictSetVal(d, entry, val);    return DICT_OK;&#125;\n\ndictAddRawdictEntry *dictAddRaw(dict *d, void *key, dictEntry **existing)&#123;    long index;    dictEntry *entry;    dictht *ht;    // 该字典是否在进行 rehash 操作，是则执行一次 rehash    if (dictIsRehashing(d)) _dictRehashStep(d);    // 查询某个Key的索引下标,其实就是查询该key是否存在    // 已存在返回-1    if ((index = _dictKeyIndex(d, key, dictHashKey(d,key), existing)) == -1)        return NULL; \t// 是否在进行 rehash 操作中，是则插入至散列表 ht[1] 中，否则插入散列表 ht[0]     ht = dictIsRehashing(d) ? &amp;d-&gt;ht[1] : &amp;d-&gt;ht[0];    entry = zmalloc(sizeof(*entry)); // 申请新节点内存    entry-&gt;next = ht-&gt;table[index]; // 将该节点的 next 指针指向 ht-&gt;table[index] 指针指向的位置    ht-&gt;table[index] = entry; // 将 ht-&gt;table[index] 指针指向该节点    ht-&gt;used++;    /* Set the hash entry fields. */    dictSetKey(d, entry, key);    return entry;&#125;\n\n其中的 rehash 会在扩容中讲述。\n最关键的一个点：ht = dictIsRehashing(d) ? &amp;d-&gt;ht[1] : &amp;d-&gt;ht[0];。在进行 rehash放入 ht[1]反之则 ht[0]\n查询操作dictFetchValuevoid *dictFetchValue(dict *d, const void *key) &#123;    dictEntry *he;    // 关键在这一步,查询对应key所在的节点    he = dictFind(d,key);    // 根据节点查询里面的value,否则返回NULL    return he ? dictGetVal(he) : NULL;&#125;\n\ndictFinddictEntry *dictFind(dict *d, const void *key)&#123;    dictEntry *he;    uint64_t h, idx, table;    // 为空    if (dictSize(d) == 0) return NULL; /* dict is empty */    // 该字典是否在进行 rehash 操作，是则执行一次 rehash    if (dictIsRehashing(d)) _dictRehashStep(d);    // 根据字典的hash函数得到key的hash值    h = dictHashKey(d, key);    // 循环两次,分别在ht[0]和ht[1]中查找    for (table = 0; table &lt;= 1; table++) &#123;        // 根据hash值与掩码取table中的下标        idx = h &amp; d-&gt;ht[table].sizemask;        // 获取到对应Entity        he = d-&gt;ht[table].table[idx];        // 如果出现了hash冲突会逐个遍历链表        while(he) &#123;            // 判断key值是不是要查询的目标值            if (key==he-&gt;key || dictCompareKeys(d, key, he-&gt;key))                return he;            // 遍历下一个元素            he = he-&gt;next;        &#125;        if (!dictIsRehashing(d)) return NULL;    &#125;    return NULL;&#125;\n\n几个关键点：\n\nidx = h &amp; d-&gt;ht[table].sizemask; ：在日常开发中这种场景我们更喜欢使用取余（%），因为快、方便；在HashMap中使用的就是取余，但是限制了数组长度为2的幂+位运算快速取余；在Redis中则是使用了掩码进行 &amp;运算\n\ndictGetVal#define dictGetVal(he) ((he)-&gt;v.val)\n\n扩容操作其实在前面的操作中已经看到了一些关于扩容的影子，例如什么是 rehash？为什么要有 ht[1]和 ht[2]\n其实这些都是为了实现一个机制——渐进式扩容。\n什么是渐进式扩容就好像一口吃不成胖子，一口气吃太多吃太快对胃不好，要慢慢吃慢慢长胖\n\nRedis中Hash类型使用了渐进式的扩容机制，因为遇到大Key时会对性能造成巨大的影响。所以在dict对象中有这样一个属性：dictht ht[2] ，存放了一个长度为2的数组。\nht[0]，是存放数据的 table，作为非扩容时容器；\nht[1]，只有正在进行扩容时才会使用，它也是存放数据的table，长度为 ht[0]的两倍。\n扩容时，单线程A负责把数据从 ht[0] 复制到 ht[1] 中。如果这时有其他线程进行读操作：会先去 ht[0]中找，找不到再去 ht[1]中找。进行写操作：直接写在 ht[1]中。进行删除操作：与读类似。\n\n扩容流程\nint _dictExpand(dict *d, unsigned long size, int* malloc_failed)&#123;    if (malloc_failed) *malloc_failed = 0;    //  如果此时正在扩容，或者是扩容大小小于 ht[0] 的表大小，则抛错    if (dictIsRehashing(d) || d-&gt;ht[0].used &gt; size)        return DICT_ERR;    dictht n;  // new了一个新的hash表    unsigned long realsize = _dictNextPower(size);    // 是否存在溢出可能    if (realsize &lt; size || realsize * sizeof(dictEntry*) &lt; realsize)        return DICT_ERR;    // 重新计算的值如果和原来的 size 相等，则无效    if (realsize == d-&gt;ht[0].size) return DICT_ERR;    // 分配新 Hash 表，并初始化所有指针为 NULL    n.size = realsize;    n.sizemask = realsize-1;    if (malloc_failed) &#123;        n.table = ztrycalloc(realsize*sizeof(dictEntry*));        *malloc_failed = n.table == NULL;        if (*malloc_failed)            return DICT_ERR;    &#125; else        n.table = zcalloc(realsize*sizeof(dictEntry*));    n.used = 0;    // 初始化的情况，而不是进行 rehash 操作，就用 ht[0] 来接收值    if (d-&gt;ht[0].table == NULL) &#123;        // 如果是初始化阶段,赋值后直接就结束了        d-&gt;ht[0] = n;        return DICT_OK;    &#125;    /* 准备第二个 Hash 表，以便执行渐进式哈希操作 */    d-&gt;ht[1] = n;    d-&gt;rehashidx = 0;    return DICT_OK;&#125;\n\n\nredis中的key可能有成千上万，如果一次性扩容，会对性能造成巨大的影响，所以redis使用渐进式扩容，每次执行插入，删除，查找，修改等操作前，都先判断当前字典的rehash操作是否在进行，如果是在进行中，就对当前节点进行rehash操作，只执行一次。除此之外，当服务器空闲时，也会调用incrementallyRehash函数进行批量操作，每次100个节点，大概一毫秒。将rehash操作进行分而治之。\n\n渐进式rehash流程rehash的准备工作\n设置字典的 rehashidx 为 0 ，标识着 rehash 的开始；\n为 ht[1]-&gt;table 分配空间，大小至少为 ht[0]-&gt;used 的两倍；\n\ndictRehashint dictRehash(dict *d, int n) &#123;    // 一次最多rehash n*10个空桶 n的大小根据触发场景而定    // 在添加新的元素时也会触发Rehash操作 _dictRehashStep n=1,一次最多rehash10个空桶    // dictRehashMilliseconds 在给定毫秒内，对字典进行rehash的一个方法,n=100,一次最多1000个空桶    int empty_visits = n*10;     // 如果不在Rehash则返回0    if (!dictIsRehashing(d)) return 0;\t// 开始循环，执行    while(n-- &amp;&amp; d-&gt;ht[0].used != 0) &#123;        dictEntry *de, *nextde;        // 为防止 rehashidx 越界，当 rehashidx 大于 ht[0] 的数组大小时，不继续执行         assert(d-&gt;ht[0].size &gt; (unsigned long)d-&gt;rehashidx);        // d-&gt;rehashidx 是记录着上一次rehash进行到了哪一个索引        // 一直往下遍历,直到遇到不是空桶的就结束(前提是没有因为空桶数而终止)        while(d-&gt;ht[0].table[d-&gt;rehashidx] == NULL) &#123;            // 如果为空表示当前索引rehash成功 +1            // 并且空桶数+1            d-&gt;rehashidx++;            if (--empty_visits == 0) return 1;        &#125;        // 这个桶一定是有数据的        de = d-&gt;ht[0].table[d-&gt;rehashidx];        // 遍历桶中元素，移动元素至新表        while(de) &#123;            uint64_t h;            nextde = de-&gt;next;            // 获取在新桶时的下标            h = dictHashKey(d, de-&gt;key) &amp; d-&gt;ht[1].sizemask;            // 此处使用了头插法            de-&gt;next = d-&gt;ht[1].table[h];             d-&gt;ht[1].table[h] = de;             d-&gt;ht[0].used--;            d-&gt;ht[1].used++;            de = nextde;        &#125;        // 处理完成 设置为空        d-&gt;ht[0].table[d-&gt;rehashidx] = NULL;        d-&gt;rehashidx++;    &#125;    // 检查是否已经 rehash 完成    if (d-&gt;ht[0].used == 0) &#123;        zfree(d-&gt;ht[0].table);        d-&gt;ht[0] = d-&gt;ht[1];        _dictReset(&amp;d-&gt;ht[1]);        d-&gt;rehashidx = -1;        return 0;    &#125;    /* More to rehash... */    return 1;&#125;\n\n\n首先梳理 dictRehash方法，N和N*10的关系。\n\n一次 dictRehash方法最多迁移N个桶。\n但是每次 dictRehash不一定都是需要迁移的，可能有些桶的数据为NULL，遇到N*10次空桶则结束（为什么这么做？不能无休止的让他遍历下去，防止过久的阻塞）\n\n\n关于头插法\n\nde-&gt;next = d-&gt;ht[1].table[h]; 首先将 ht1中的de对象的尾节点\n再将de对象设置到 d-&gt;ht[1].table[h]\n\n\n\n总结\nRedis 中的数据库和哈希键都基于字典来实现。\n\n字典是由键值对构成的抽象数据结构。\n\nRedis自己实现了一套哈希表的逻辑，与Java中的HashMap相似但是不完全一样，拿两者比较的总结一下。\n\n两者都通过拉链法解决了Hash冲突问题，只不过HashMap有升级红黑树的机制，Redis没有\nRedis实现了逐渐式扩容，不会一下子把所有Key进行迁移（遇到大Key可能出现异常）；HashMap就是普通的迁移。\n逐渐式扩容使Redis底层需要使用到两个Hash表，一般情况下只使用 0 号哈希表，只有在 rehash 进行时，才会同时使用 0 号和 1 号哈希表。\n\n\nHashMap通过指定规则：数组长度为2的幂+位运算进行取数组下标；Redis使用掩码的方式。\nHashMap在1.8之前为头插法，之后为尾插法；Redis中一直为头插法。\n\n\n\n致谢\nRedis源码解析 - 有赞技术团队\nRedis 设计与实现\n\n","categories":["Redis"],"tags":["源码","Redis数据结构源码解析系列"]},{"title":"Redis数据结构源码解析(五) 双向链表","url":"/2022/01/19/Redis%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E4%BA%94-%E5%8F%8C%E5%90%91%E9%93%BE%E8%A1%A8/","content":"前言在Redis3.2版本之前，List 类型的实现是由：压缩列表+双向链表实现的，在3.2版本之后取缔了双向链表。\n取缔的原因也很简单，在压缩列表中有讲到。\n基本结构其实双向链表还是很常见的，也没什么不同就是首尾相接的一个个节点。\nlistNodetypedef struct listNode &#123;    // 前驱节点    struct listNode *prev;    // 后继节点    struct listNode *next;    // 值    void *value;&#125; listNode;\n\nlisttypedef struct list &#123;    // 表头指针    listNode *head;    // 表尾指针    listNode *tail;    // 节点数量    unsigned long len;    // 复制函数    void *(*dup)(void *ptr);    // 释放函数    void (*free)(void *ptr);    // 比对函数    int (*match)(void *ptr, void *key);&#125; list;\n\n优劣分析\n双向链表linkedlist便于在表的两端进行push和pop操作，在插入节点上复杂度很低，但是它的内存开销比较大。\n它在每个节点上除了要保存数据之外，还要额外保存两个指针；\n双向链表的各个节点是单独的内存块，地址不连续，节点多了容易产生内存碎片。\n\n总结\nRedis 实现了自己的双端链表结构。\n双端链表主要有两个作用：\n作为 Redis 列表类型的底层实现之一（3.2后弃用）\n作为通用数据结构，被其他功能模块所使用；\n\n\n双端链表及其节点的性能特性如下：\n节点带有前驱和后继指针，访问前驱节点和后继节点的复杂度为 O(1) ，并且对链表的迭代可以在从表头到表尾和从表尾到表头两个方向进行；\n链表带有指向表头和表尾的指针，因此对表头和表尾进行处理的复杂度为 O(1)；\n链表带有记录节点数量的属性，所以可以在 O(1)复杂度内返回链表的节点数量（长度）；\n\n\n\n","categories":["Redis"],"tags":["源码","Redis数据结构源码解析系列"]},{"title":"Redis数据结构源码解析(六) 快速列表","url":"/2022/01/20/Redis%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E5%85%AD-%E5%BF%AB%E9%80%9F%E5%88%97%E8%A1%A8/","content":"前言在前面分别介绍了：双向链表和压缩列表，两者作为老版本中List类型的底层实现，但是在3.2版本后被本文主角取代。\n基于两者的优劣，衍生了本文的主角——快速列表\n概念quickList 实际上是 zipList 和 linkedList 的混合体，它将 linkedList 按段切分，每一段使用 zipList 来紧凑存储，多个 zipList 之间使用双向指针串接起来。\n简单来说：将多个压缩列表组成一个链表就是快速列表了。\n为什么这么做zipList 实现是基于数组，相比较双向链表最大的提升就是减少了内存的碎片，这点对于一个基于内存的高性能存储时很重要的，但是它带来的问题也很严峻，列表元素当达到一定的量级就可能出现问题：\n\n很难找到那么大的连续内存空间：如果一个压缩列表无限大，虽然服务器可用内存好友但是因为没有足够的连续内存也会出现问题。\n当列表足够大时，每一次的插入和删除操作需要频繁的申请和释放内存，产生大量的数据拷贝。\n\n所以遵循着分治原则，将一个大的列表拆分成N个小的列表通过指针首尾相接就有了—— QuickList 快速列表。\n基本结构quicklisttypedef struct quicklist &#123;    // 头结点    quicklistNode *head;    // 尾结点    quicklistNode *tail;    // 压缩列表的个数    unsigned long count;        /* total count of all entries in all ziplists */    // 快速列表的个数    unsigned long len;          /* number of quicklistNodes */    // 单个节点的填充因子    int fill : QL_FILL_BITS;              /* fill factor for individual nodes */    unsigned int compress : QL_COMP_BITS; /* depth of end nodes not to compress;0=off */    unsigned int bookmark_count: QL_BM_BITS;    quicklistBookmark bookmarks[];&#125; quicklist;\n\nquicklistNodetypedef struct quicklistNode &#123;    // 前置节点    struct quicklistNode *prev;    // 后置节点    struct quicklistNode *next;    // 主体 也就是 ziplist 压缩列表    unsigned char *zl;    /* 下面是一些节点的统计属性 */    unsigned int sz;             /* ziplist size in bytes */    unsigned int count : 16;     /* count of items in ziplist */    unsigned int encoding : 2;   /* RAW==1 or LZF==2 */    unsigned int container : 2;  /* NONE==1 or ZIPLIST==2 */    unsigned int recompress : 1; /* was this node previous compressed? */    unsigned int attempted_compress : 1; /* node can&#x27;t compress; too small */    unsigned int extra : 10; /* more bits to steal for future usage */&#125; quicklistNode;\n\n\n\nprev: 指向链表前一个节点的指针。\nnext: 指向链表后一个节点的指针。\nzl: 数据指针。如果当前节点的数据没有压缩，那么它指向一个ziplist结构；否则，它指向一个quicklistLZF结构。\nsz: 表示zl指向的ziplist的总大小（包括 zlbytes, zltail, zllen, zlend和各个数据项）。需要注意的是：如果ziplist被压缩了，那么这个sz的值仍然是压缩前的ziplist大小。\ncount: 表示ziplist里面包含的数据项个数。这个字段只有16bit。稍后我们会一起计算一下这16bit是否够用。\nencoding: 表示ziplist是否压缩了（以及用了哪个压缩算法）。目前只有两种取值：2表示被压缩了（而且用的是LZF压缩算法），1表示没有压缩。\ncontainer: 是一个预留字段。本来设计是用来表明一个quicklist节点下面是直接存数据，还是使用ziplist存数据，或者用其它的结构来存数据（用作一个数据容器，所以叫container）。但是，在目前的实现中，这个值是一个固定的值2，表示使用ziplist作为数据容器。\nrecompress: 当我们使用类似lindex这样的命令查看了某一项本来压缩的数据时，需要把数据暂时解压，这时就设置recompress=1做一个标记，等有机会再把数据重新压缩。\nattempted_compress: 这个值只对Redis的自动化测试程序有用。我们不用管它。\nextra: 其它扩展字段。目前Redis的实现里也没用上。\n\n\n基本操作插入quicklist可以选择在头部或者尾部进行插入(quicklistPushHead和 quicklistPushTail)，而不管是在头部还是尾部插入数据，都包含两种情况：\n\n如果头节点（或尾节点）上ziplist大小没有超过限制（即 _quicklistNodeAllowInsert返回1），那么新数据被直接插入到ziplist中（调用 ziplistPush）。\n如果头节点（或尾节点）上 ziplist太大了，那么新创建一个quicklistNode节点（对应地也会新创建一个ziplist），然后把这个新创建的节点插入到quicklist双向链表中。\n\n\n总结\n一个基于压缩列表和链表实现的数据结构，本质用于解决单纯的链表和压缩列表所遇到的一些内存上问题。\n\n致谢\nRedis数据结构——快速列表(quicklist) \n\n","categories":["Redis"],"tags":["源码","Redis数据结构源码解析系列"]},{"title":"Redis数据结构源码解析(四) 压缩列表","url":"/2022/01/19/Redis%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E5%9B%9B-%E5%8E%8B%E7%BC%A9%E5%88%97%E8%A1%A8/","content":"结合Java对比Redis的压缩列表说到Java中的列表，我们可能会快想到 ArrayList和 LinkedList，他们俩的区别在于一个是数组实现一个是链表实现。对于数组和链表都有各自的优劣。\n回到Redis的列表同样也可以使用数组和列表去实现，Redis中有实现 listNode 和 list 结构，使用了链表结果但是这只是用于服务端存放运行数据，不存放开发者存储的数据。\n原因很简单：\n\n链表在元素过多的时候确实可以提高插入的效率的优势，但是他对内存的管理不够优秀，会产生大量的内存碎片。内存碎片在大量执行内存操作的Redis中显然是不友好的。\n链表的 Node 结构中前置节点和后置节点指针属性太多，造成内存的浪费！（真的是把勤俭持家发挥到了极致，从来没考虑过这个东西可能会浪费多少内存）\n\n就如上两个问题在Redis中使用了数组作为压缩列表的底层实现。Redis自己定义了一个规范(或者叫格式?)，将数据整齐的安排好，有统一的插入与查询方法，保证即使列表被压缩在了数组中也能完整的读取。\n抽象结构先抽象的看一下，我前面所说的规范、格式大致是个什么样子\nlist在数组中基本以如下格式进行存储\n\n&lt;zlbytes&gt; &lt;zltail&gt; &lt;zllen&gt; &lt;entry&gt; &lt;entry&gt; ... &lt;entry&gt; &lt;zlend&gt;\n\n\nzlbytes :记录整个列表的占用字节数，也包括了自身的4个字节\nzltail：记录从第一个节点到最后一个节点的偏移量，用于反向的遍历\nzllen：节点个数，如果超过2^16 -1 时就无法记录节点个数了，此时统计需要O(N)遍历\nentry：就是存储客户端数据的节点\nzlend：一个标记位，类似于字符串中说到的 \\0，仅仅是用于记录结束点\n\nentity&lt;prevlen&gt; &lt;encoding&gt; &lt;entry-data&gt;\nprevlen：前驱节点的长度\nencoding：当前节点的编码格式\nentry-data：真正的数据\n基本结构zlentrytypedef struct zlentry &#123;    // 前置节点长度    unsigned int prevrawlensize; /* Bytes used to encode the previous entry len*/    // 前置    unsigned int prevrawlen;     /* Previous entry len. */    // encoding 长度    unsigned int lensize;        /* Bytes used to encode this entry type/len.                                    For example strings have a 1, 2 or 5 bytes                                    header. Integers always use a single byte.*/    // 内容的长度    unsigned int len;            /* Bytes used to represent the actual entry.                                    For strings this is just the string length                                    while for integers it is 1, 2, 3, 4, 8 or                                    0 (for 4 bit immediate) depending on the                                    number range. */    // 首部长度    unsigned int headersize;     /* prevrawlensize + lensize. */    // 编码    unsigned char encoding;      /* Set to ZIP_STR_* or ZIP_INT_* depending on                                    the entry encoding. However for 4 bits                                    immediate integers this can assume a range                                    of values and must be range-checked. */    // 当前元素的首地址    unsigned char *p;            /* Pointer to the very start of the entry, that                                    is, this points to prev-entry-len field. */&#125; zlentry;\n\nTODO\n暂时先抽象了解，后续补上\n\n优劣分析ziplist 存储在一段连续的内存上，所以存储效率很高。但是，它不利于修改操作，插入和删除操作需要频繁的申请和释放内存。特别是当ziplist长度很长的时候，一次 realloc可能会导致大批量的数据拷贝。\n总结\n关于列表的底层实现，Java圈里貌似更多还是喜欢用链表，但是Redis使用了数组，原因是：链表会造成内存的碎片和过多的节点指针造成内存的浪费。\n\n致谢\nRedis源码解析 - 有赞技术团队\nRedis 设计与实现\n\n","categories":["Redis"],"tags":["源码","Redis数据结构源码解析系列"]},{"title":"ReentrantLock 公平锁与非公平锁","url":"/2022/01/06/ReentrantLock%20%E5%85%AC%E5%B9%B3%E9%94%81%E4%B8%8E%E9%9D%9E%E5%85%AC%E5%B9%B3%E9%94%81/","content":"实例化在实例化 ReentrantLock 的时候可以加一个构造参数。\n// 传true时为公平锁ReentrantLock lock = new ReentrantLock(true);// 非公平锁ReentrantLock lock = new ReentrantLock();\n\n当然他们的构造方法也是不一样的\n构造方法// fair为true时为公平锁public ReentrantLock(boolean fair) &#123;    sync = fair ? new FairSync() : new NonfairSync();&#125;// 默认非公平锁public ReentrantLock() &#123;    sync = new NonfairSync();&#125;\n\n此事我们发现 ReentrantLock 为了应对公平锁与非公平锁实现了两个类： FairSync 和 NonfairSync 。他们都是继承于 Sync 类。\nlock我们找到 FairSync 和 NonfairSync  类的 lock 方法，发现区别还是很明显的。\nstatic final class FairSync extends Sync &#123;    final void lock() &#123;        acquire(1);    &#125;&#125;static final class NonfairSync extends Sync &#123;     final void lock() &#123;         // 发现非公平锁比公平锁多了一步         if (compareAndSetState(0, 1))             setExclusiveOwnerThread(Thread.currentThread());         else             acquire(1);     &#125;&#125;\n\n公平锁在执行 lock 方法时会直接执行 acquire 方法，而非公平锁会先尝试替换一下状态，如果成功则设置当前线程为所有者。\ntryAcquire// 公平锁protected final boolean tryAcquire(int acquires) &#123;    final Thread current = Thread.currentThread();    int c = getState();    if (c == 0) &#123;        if (!hasQueuedPredecessors() &amp;&amp;            compareAndSetState(0, acquires)) &#123;            setExclusiveOwnerThread(current);            return true;        &#125;    &#125;    else if (current == getExclusiveOwnerThread()) &#123;        int nextc = c + acquires;        if (nextc &lt; 0)            throw new Error(&quot;Maximum lock count exceeded&quot;);        setState(nextc);        return true;    &#125;    return false;&#125;// 非公平锁protected final boolean tryAcquire(int acquires) &#123;    return nonfairTryAcquire(acquires);&#125;final boolean nonfairTryAcquire(int acquires) &#123;    final Thread current = Thread.currentThread();    int c = getState();    if (c == 0) &#123;        if (compareAndSetState(0, acquires)) &#123;            setExclusiveOwnerThread(current);            return true;        &#125;    &#125;    else if (current == getExclusiveOwnerThread()) &#123;        int nextc = c + acquires;        if (nextc &lt; 0) // overflow            throw new Error(&quot;Maximum lock count exceeded&quot;);        setState(nextc);        return true;    &#125;    return false;&#125;\n\n貌似很多，是否公平的关键只是一个方法。\n// 非公平if (c == 0) &#123;    if (compareAndSetState(0, acquires)) &#123;        setExclusiveOwnerThread(current);        return true;    &#125;&#125;// 公平if (c == 0) &#123;    // 多了一个hasQueuedPredecessors    if (!hasQueuedPredecessors() &amp;&amp;        compareAndSetState(0, acquires)) &#123;        setExclusiveOwnerThread(current);        return true;    &#125;&#125;\n\nhasQueuedPredecessors翻译过来就是队列是否有前节点。\npublic final boolean hasQueuedPredecessors() &#123;    // The correctness of this depends on head being initialized    // before tail and on head.next being accurate if the current    // thread is first in queue.    Node t = tail; // Read fields in reverse initialization order    Node h = head;    Node s;    return h != t &amp;&amp;        ((s = h.next) == null || s.thread != Thread.currentThread());&#125;\n\n头节点不是尾节点 且 （ 头节点的下一个节点不为空 或者 头节点的下一个节点不是当前节点 ）\n","categories":["Java"],"tags":["多线程与高并发系列","AQS","锁","ReentrantLock"]},{"title":"ReentrantLock 前言","url":"/2022/01/06/ReentrantLock%20%E5%89%8D%E8%A8%80/","content":"前言ReentrantLock 和 synchronized 都是 Java 语言中常见的锁，在软件行业没有永远的银弹既然存在表示有存在的意义。\n这就要从2004年说起，当时的 synchronized 还没有经过优化是一个重量级锁，所以同年发布的 JDK1.5 发布了 ReentrantLock ，从此开发者面对锁又多了一个选择。两年后的2006年 JDK1.6 发布，发布了优化后的 synchronized ，并且带来了无锁、偏向锁、锁升级等概念，使得其效率得到提升。\n虽然性能得到了提升，但有一点的 ReentrantLock 还是要更为领先的，那就是灵活性。sync是Java中的关键字，就如同常见的 Interface、final等，而 ReentrantLock 是 JUC 包下的一个对象，灵活性让他带来了 tryLock()、公平锁、死锁检测、配合 try...finally使用等特性。\n","categories":["Java"],"tags":["多线程与高并发系列","AQS","锁","ReentrantLock"]},{"title":"ReentrantLock 如何保证重入","url":"/2022/01/06/ReentrantLock%20%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E9%87%8D%E5%85%A5/","content":"公平锁：// java.util.concurrent.locks.ReentrantLock.FairSync#tryAcquireif (c == 0) &#123;\tif (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123;\t\tsetExclusiveOwnerThread(current);\t\treturn true;\t&#125;&#125;else if (current == getExclusiveOwnerThread()) &#123;\tint nextc = c + acquires;\tif (nextc &lt; 0)\t\tthrow new Error(&quot;Maximum lock count exceeded&quot;);\tsetState(nextc);\treturn true;&#125;\n\n非公平锁：// java.util.concurrent.locks.ReentrantLock.Sync#nonfairTryAcquireif (c == 0) &#123;\tif (compareAndSetState(0, acquires))&#123;\t\tsetExclusiveOwnerThread(current);\t\treturn true;\t&#125;&#125;else if (current == getExclusiveOwnerThread()) &#123;\tint nextc = c + acquires;\tif (nextc &lt; 0) // overflow\t\tthrow new Error(&quot;Maximum lock count exceeded&quot;);\tsetState(nextc);\treturn true;&#125;\n\n从上面这两段都可以看到，有一个同步状态State来控制整体可重入的情况。State是 Volatile修饰的，用于保证一定的可见性和有序性。\n// java.util.concurrent.locks.AbstractQueuedSynchronizerprivate volatile int state;\n\n接下来看 State这个字段主要的过程：\n\nState初始化的时候为0，表示没有任何线程持有锁。\n当有线程持有该锁时，值就会在原来的基础上+1，同一个线程多次获得锁是，就会多次+1，这里就是可重入的概念。\n解锁也是对这个字段-1，一直到0，此线程对锁释放。\n\n","categories":["Java"],"tags":["多线程与高并发系列","AQS","锁","ReentrantLock"]},{"title":"ReentrantLock 源码分析 - 加锁","url":"/2022/01/06/ReentrantLock%20%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%20-%20%E5%8A%A0%E9%94%81/","content":"前言本系列文章通过日常中比较常见 ReentrantLock 开始，逐步揭开AQS的底层。\nlock()ReentrantLock lock = new ReentrantLock();try&#123;    lock.lock();    // 业务...&#125;finally &#123;    lock.unlock();&#125;\n\n日常我们会这样使用 ReentrantLock\n// java.util.concurrent.locksfinal void lock() &#123;    // 首先做了一个判断    // 比较并替换(CAS) 如果期望值为0则替换为1    if (compareAndSetState(0, 1))        // 如果替换成功        // 表示已经拿到了锁        setExclusiveOwnerThread(Thread.currentThread());    else        // 否则将执行一个获取锁的操作        acquire(1);&#125;\n\n上面代码中 compareAndSetState  、 setExclusiveOwnerThread 、 acquire 三个方法都是AQS下的。\nacquire()public final void acquire(int arg) &#123;    if (!tryAcquire(arg) &amp;&amp;        acquireQueued(addWaiter(Node.EXCLUSIVE), arg))        selfInterrupt();&#125;\n\naddWaiterprivate Node addWaiter(Node mode) &#123;    // 新建了一个Node对象    Node node = new Node(Thread.currentThread(), mode);    // 拿到尾巴节点    Node pred = tail;    if (pred != null) &#123;        node.prev = pred;        // 将新创建的节点追加到链表的尾巴上        // 此处同样用到了比较替换        // 这个方法主要是对tailOffset和Expect进行比较        // 如果tailOffset的Node和Expect的Node地址是相同的，那么设置Tail的值为Update的值        if (compareAndSetTail(pred, node)) &#123;            pred.next = node;            // 最终返回当前节点            return node;        &#125;    &#125;    // 内部是一个死循环，直到设置node为尾节点为止。    enq(node);    return node;&#125;\n\n\n关于 compareAndSetTail 方法深挖可以发现他是一个 native 修饰的方法。高并发下难免会出现多线程并发去追加尾巴节点，防止追加错乱。使用了CAS没有使用其他锁是因为，CAS的颗粒比较小，永远只需要盯着 pred 作比较，无需锁住整个链表。\n\nacquireQueued上文解释了 addWaiter 方法，这个方法其实就是把对应的线程以Node的数据结构形式加入到双端队列里，返回的是一个包含该线程的Node。而这个Node会作为参数，进入到 acquireQueued 方法中。 acquireQueued 方法可以对排队中的线程进行“获锁”操作。总的来说，一个线程获取锁失败了，被放入等待队列， acquireQueued 会把放入队列中的线程不断去获取锁，直到获取成功或者不再需要获取（中断）。下面我们从“何时出队列？”和“如何出队列？”两个方向来分析一下acquireQueued源码：\n// java.util.concurrent.locks.AbstractQueuedSynchronizerfinal boolean acquireQueued(final Node node, int arg) &#123;\t// 标记是否成功拿到资源\tboolean failed = true;\ttry &#123;\t\t// 标记等待过程中是否中断过\t\tboolean interrupted = false;\t\t// 开始死循环(轮询)        // 直至拿到锁,或者中断\t\tfor (;;) &#123;\t\t\t// 获取当前节点的前置节点\t\t\tfinal Node p = node.predecessor();\t\t\t// 如果p是头结点，说明当前节点在真实数据队列的首部，就尝试获取锁（别忘了头结点是虚节点）\t\t\tif (p == head &amp;&amp; tryAcquire(arg)) &#123;\t\t\t\t// 获取锁成功，自己作为新的头结点\t\t\t\tsetHead(node);                // p.next 其实就是 本方法传参中的 node                // 设置为null 没有了引用 有利于gc 清理node\t\t\t\tp.next = null; // help GC \t\t\t\tfailed = false;\t\t\t\treturn interrupted;\t\t\t&#125;\t\t\t// 说明p为头节点且当前没有获取到锁（可能是非公平锁被抢占了）或者是p不为头结点            // 这个时候就要判断当前node是否要被阻塞（被阻塞条件：前驱节点的waitStatus为-1），防止无限循环浪费资源\t\t\tif (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt())\t\t\t\tinterrupted = true;\t\t&#125;\t&#125; finally &#123;\t\tif (failed)\t\t\tcancelAcquire(node);\t&#125;&#125;\n\nshouldParkAfterFailedAcquire// java.util.concurrent.locks.AbstractQueuedSynchronizer// 靠前驱节点判断当前线程是否应该被阻塞private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123;\t// 获取头结点的节点状态\tint ws = pred.waitStatus;\t// 说明头结点处于唤醒状态\tif (ws == Node.SIGNAL)\t\treturn true; \t// 通过枚举值我们知道waitStatus&gt;0是取消状态\tif (ws &gt; 0) &#123;\t\tdo &#123;\t\t\t// 循环向前查找取消节点，把取消节点从队列中剔除\t\t\tnode.prev = pred = pred.prev;\t\t&#125; while (pred.waitStatus &gt; 0);\t\tpred.next = node;\t&#125; else &#123;\t\t// 设置前任节点等待状态为SIGNAL\t\tcompareAndSetWaitStatus(pred, ws, Node.SIGNAL);\t&#125;\treturn false;&#125;\n\nparkAndCheckInterruptprivate final boolean parkAndCheckInterrupt() &#123;    LockSupport.park(this);    return Thread.interrupted();&#125;\n\nparkAndCheckInterrupt主要用于挂起当前线程，阻塞调用栈，返回当前线程的中断状态。\n总结从上图可以看出，跳出当前循环的条件是当“前置节点是头结点，且当前线程获取锁成功”。为了防止因死循环导致CPU资源被浪费，我们会判断前置节点的状态来决定是否要将当前线程挂起，具体挂起流程用流程图表示如下（shouldParkAfterFailedAcquire流程）：\n\n从队列中释放节点的疑虑打消了，那么又有新问题了：\n\nshouldParkAfterFailedAcquire中取消节点是怎么生成的呢？什么时候会把一个节点的waitStatus设置为-1？\n是在什么时间释放节点通知到被挂起的线程呢？\n\ncancelAcquire通过cancelAcquire方法，将Node的状态标记为CANCELLED。接下来，我们逐行来分析这个方法的原理：\n// java.util.concurrent.locks.AbstractQueuedSynchronizerprivate void cancelAcquire(Node node) &#123;    // 将无效节点过滤    if (node == null)        return;    // 设置该节点不关联任何线程，也就是虚节点    node.thread = null;    Node pred = node.prev;    // 通过前驱节点，跳过取消状态的node    while (pred.waitStatus &gt; 0)        node.prev = pred = pred.prev;    // 获取过滤后的前驱节点的后继节点    Node predNext = pred.next;    // 把当前node的状态设置为CANCELLED    node.waitStatus = Node.CANCELLED;    // 如果当前节点是尾节点，将从后往前的第一个非取消状态的节点设置为尾节点    // 更新失败的话，则进入else，如果更新成功，将tail的后继节点设置为null    if (node == tail &amp;&amp; compareAndSetTail(node, pred)) &#123;        compareAndSetNext(pred, predNext, null);    &#125; else &#123;        int ws;        // 如果当前节点不是head的后继节点，1:判断当前节点前驱节点的是否为SIGNAL，2:如果不是，则把前驱节点设置为SINGAL看是否成功        // 如果1和2中有一个为true，再判断当前节点的线程是否为null        // 如果上述条件都满足，把当前节点的前驱节点的后继指针指向当前节点的后继节点        if (pred != head &amp;&amp; ((ws = pred.waitStatus) == Node.SIGNAL || (ws &lt;= 0 &amp;&amp; compareAndSetWaitStatus(pred, ws, Node.SIGNAL))) &amp;&amp; pred.thread != null) &#123;            Node next = node.next;            if (next != null &amp;&amp; next.waitStatus &lt;= 0)                compareAndSetNext(pred, predNext, next);        &#125; else &#123;            // 如果当前节点是head的后继节点，或者上述条件不满足，那就唤醒当前节点的后继节点            unparkSuccessor(node);        &#125;        node.next = node; // help GC    &#125;&#125;\n","categories":["Java"],"tags":["多线程与高并发系列","AQS","锁","ReentrantLock"]},{"title":"ReentrantLock 源码分析 - 解锁","url":"/2022/01/06/ReentrantLock%20%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%20-%20%E8%A7%A3%E9%94%81/","content":"releasepublic final boolean release(int arg) &#123;    // 尝试释放    if (tryRelease(arg)) &#123;        Node h = head;        // 头结点不为空 且 头结点状态不为0        if (h != null &amp;&amp; h.waitStatus != 0)            // 唤醒后节点            unparkSuccessor(h);        return true;    &#125;    return false;&#125;\n\ntryRelease@ReservedStackAccessprotected final boolean tryRelease(int releases) &#123;    // 先判断锁计数器    int c = getState() - releases;    // 是否是当前线程持有锁    if (Thread.currentThread() != getExclusiveOwnerThread())        throw new IllegalMonitorStateException();    // 默认返回的是false    boolean free = false;    // 如果锁计数器为0 表示重入锁全部解锁    if (c == 0) &#123;        // 设置为true        free = true;        // 设置锁所有人为空        setExclusiveOwnerThread(null);    &#125;    // 设置锁状态    setState(c);    return free;&#125;\n\nunparkSuccessorprivate void unparkSuccessor(Node node) &#123;   // 如果当前节点状态 为-1 -2 -3 则通过比较替换为0    int ws = node.waitStatus;    if (ws &lt; 0)        compareAndSetWaitStatus(node, ws, 0);    Node s = node.next;    // 如果下个节点是null或者下个节点被cancelled    // 就链表一直往下找,直至找到状态小于等同于0    if (s == null || s.waitStatus &gt; 0) &#123;        s = null;        for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev)            if (t.waitStatus &lt;= 0)                s = t;    &#125;    // 如果当前节点的下个节点不为空，而且状态&lt;=0，就把当前节点unpark    if (s != null)        LockSupport.unpark(s.thread);&#125;\n","categories":["Java"],"tags":["多线程与高并发系列","AQS","锁","ReentrantLock"]},{"title":"Seata 分布式事务原理源码分析（一）UndoLog","url":"/2022/06/30/Seata%20%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E5%8E%9F%E7%90%86%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89UndoLog/","content":"Seata 分布式事务原理源码分析（一）UndoLog什么是UndoLog？UndoLog 是 MySQL 中比较重要的事务日志之一，顾名思义是一种用于撤销回退的日志，在事务没提交之前，MySQL会先记录更新前的数据到 UndoLog 日志文件里面，当事务回滚时或者数据库崩溃时，可以利用 UndoLog 来进行回退。\n主要作用有两个：\n\n提供回滚操作，保证原子性\n\nMySQL事务的连个操作：commit 和 rollback；rollback\n\n\n提供多版本并发控制，保证隔离型\n\n多个事务在操作时互不影响\n\n\n\nUndoLog 存在 MySQL 的服务端中，面对微服务架构做不到共享。所以 Seata AT 模式通过 UndoLog 表实现分布式事务。\nCREATE TABLE `undo_log` (  `branch_id` bigint(20) NOT NULL COMMENT &#x27;branch transaction id&#x27;,  `xid` varchar(128) NOT NULL COMMENT &#x27;global transaction id&#x27;,  `context` varchar(128) NOT NULL COMMENT &#x27;undo_log context,such as serialization&#x27;,  `rollback_info` longblob NOT NULL COMMENT &#x27;rollback info&#x27;,  `log_status` int(11) NOT NULL COMMENT &#x27;0:normal status,1:defense status&#x27;,  `log_created` datetime(6) NOT NULL COMMENT &#x27;create datetime&#x27;,  `log_modified` datetime(6) NOT NULL COMMENT &#x27;modify datetime&#x27;,  UNIQUE KEY `ux_undo_log` (`xid`,`branch_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT=&#x27;AT transaction mode undo table&#x27;;\n\n\n\nUndoLogManagerUndoLog 统一由 UndoLogManager 接口管理，实现类有：MySQLUndoLogManager 和 OracleUndoLogManager ，对应了MySQL 和 Oracle 两大数据库。\nprivate static final String INSERT_UNDO_LOG_SQL = &quot;INSERT INTO &quot; + UNDO_LOG_TABLE_NAME +            &quot; (&quot; + ClientTableColumnsName.UNDO_LOG_BRANCH_XID + &quot;, &quot; + ClientTableColumnsName.UNDO_LOG_XID + &quot;, &quot;            + ClientTableColumnsName.UNDO_LOG_CONTEXT + &quot;, &quot; + ClientTableColumnsName.UNDO_LOG_ROLLBACK_INFO + &quot;, &quot;            + ClientTableColumnsName.UNDO_LOG_LOG_STATUS + &quot;, &quot; + ClientTableColumnsName.UNDO_LOG_LOG_CREATED + &quot;, &quot;            + ClientTableColumnsName.UNDO_LOG_LOG_MODIFIED + &quot;)&quot; +            &quot; VALUES (?, ?, ?, ?, ?, now(), now())&quot;;    private static final String DELETE_UNDO_LOG_BY_CREATE_SQL = &quot;DELETE FROM &quot; + UNDO_LOG_TABLE_NAME +            &quot; WHERE log_created &lt;= ? LIMIT ?&quot;;\n\n以MySQL为例,存储了两条SQL语句，一条是插入一条是删除。分别用于开始全局事务后数据发生变动插入UndoLog，和事物提交或回滚后删除对应的UndoLog。\n何时插入UndoLog@Overridepublic void flushUndoLogs(ConnectionProxy cp) throws SQLException &#123;  ConnectionContext connectionContext = cp.getContext();  String xid = connectionContext.getXid();  long branchID = connectionContext.getBranchId();  BranchUndoLog branchUndoLog = new BranchUndoLog();  branchUndoLog.setXid(xid);  branchUndoLog.setBranchId(branchID);  branchUndoLog.setSqlUndoLogs(connectionContext.getUndoItems());  UndoLogParser parser = UndoLogParserFactory.getInstance();  byte[] undoLogContent = parser.encode(branchUndoLog);  if (LOGGER.isDebugEnabled()) &#123;    LOGGER.debug(&quot;Flushing UNDO LOG: &#123;&#125;&quot;, new String(undoLogContent, Constants.DEFAULT_CHARSET));  &#125;  insertUndoLogWithNormal(xid, branchID, buildContext(parser.getName()), undoLogContent,                          cp.getTargetConnection());&#125;\n\nprivate void insertUndoLog(String xid, long branchID, String rollbackCtx,                           byte[] undoLogContent, State state, Connection conn) throws SQLException &#123;  PreparedStatement pst = null;  try &#123;    pst = conn.prepareStatement(INSERT_UNDO_LOG_SQL);    pst.setLong(1, branchID);    pst.setString(2, xid);    pst.setString(3, rollbackCtx);    pst.setBlob(4, BlobUtils.bytes2Blob(undoLogContent));    pst.setInt(5, state.getValue());    pst.executeUpdate();  &#125; catch (Exception e) &#123;    if (!(e instanceof SQLException)) &#123;      e = new SQLException(e);    &#125;    throw (SQLException) e;  &#125; finally &#123;    if (pst != null) &#123;      pst.close();    &#125;  &#125;&#125;\n\n\n\n\n\n\n那么有个疑问，Seata是如何实现在增删改操作时自动插入UndoLog数据。\n\n","categories":["Java"],"tags":["分布式","Seata"]},{"title":"Spring源码解析(一) 创建Bean工厂","url":"/2022/01/20/Spring%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E4%B8%80-%E5%88%9B%E5%BB%BABean%E5%B7%A5%E5%8E%82/","content":"前言ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(&quot;application.xml&quot;);User userEntity = (User) context.getBean(&quot;userEntity&quot;);\n\n通过 application.xml 定义一些Bean对象，在项目启动后会由Spring的Bean工厂进行管理。\n这一切都要从 refresh 说起。\nConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory();\n\nobtainFreshBeanFactory() 主要做了两件事:\n\n创建了一个容器对象——DefaultListableBeanFactory\n\n解析自定义的Bean对象（XML、注解等），设置到 beanDefinition中\n\n\n抽象流程\nXML的解析过程： String -&gt; Resource[] -&gt; EncodedResource -&gt;Document-&gt;BeanDefinition\n创建容器对象/*** This implementation performs an actual refresh of this context&#x27;s underlying* bean factory, shutting down the previous bean factory (if any) and* initializing a fresh bean factory for the next phase of the context&#x27;s lifecycle.*/@Overrideprotected final void refreshBeanFactory() throws BeansException &#123;    // 如果存在beanFactory，则销毁beanFactory    if (hasBeanFactory()) &#123;        destroyBeans();        closeBeanFactory();    &#125;    try &#123;        // 创建DefaultListableBeanFactory对象        DefaultListableBeanFactory beanFactory = createBeanFactory();        // 为了序列化指定id，可以从id反序列化到beanFactory对象        beanFactory.setSerializationId(getId());        // 定制beanFactory，设置相关属性，包括是否允许覆盖同名称的不同定义的对象以及循环依赖        customizeBeanFactory(beanFactory);        // 初始化documentReader,并进行XML文件读取及解析,默认命名空间的解析，自定义标签的解析        loadBeanDefinitions(beanFactory);        // 将创建的Bean工厂设置到对象属性中        this.beanFactory = beanFactory;    &#125;    catch (IOException ex) &#123;        throw new ApplicationContextException(&quot;I/O error parsing bean definition source for &quot; + getDisplayName(), ex);    &#125;&#125;\n\nrefreshBeanFactory首先会创建一个BeanFactory对象，并做一些基础的配置，最后加载BeanDefinition\nprotected DefaultListableBeanFactory createBeanFactory() &#123;    return new DefaultListableBeanFactory(getInternalParentBeanFactory());&#125;\n\nDefaultListableBeanFactory就是最终的BeanFactory对象。\n解析自定义Bean对象refreshBeanFactory()中有一个loadBeanDefinitions()，初始化documentReader对XML文件进行读取，最终得到BeanDefinitions。\nBeanDefinitions是什么？\n虽然通过Reader对象读取解析得到了BeanDefinitions，但是它并不是真正意义上的Bean对象。\n可以理解为是一个 XML 到 Bean 的一个中间状态，无论你的Bean通过什么方式定义只要你能解析为BeanDefinitions我都可以把他创建成一个Spring Bean。\n\nloadBeanDefinitions@Overrideprotected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException &#123;    // 创建一个xml的beanDefinitionReader，并通过回调设置到beanFactory中    XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory);\t    // 一些配置操作,省略        // 开始完成beanDefinition的加载    loadBeanDefinitions(beanDefinitionReader);&#125;\n\nloadBeanDefinitions方法中创建了XmlBeanDefinitionReader后续会通过Reader对象对XML进行一个读取和解析。\n并且loadBeanDefinitions运用了重载，虽然还是那个名字但是参数发生了改变。参数从Bean工厂变成了Reader\nprotected void loadBeanDefinitions(XmlBeanDefinitionReader reader) throws BeansException, IOException &#123;    // 以Resource的方式获得配置文件的资源位置    Resource[] configResources = getConfigResources();    if (configResources != null) &#123;        reader.loadBeanDefinitions(configResources);    &#125;    // 以String的形式获得配置文件的位置    String[] configLocations = getConfigLocations();    if (configLocations != null) &#123;        reader.loadBeanDefinitions(configLocations);    &#125;&#125;\n\n此处就要看你在new ClassPathXmlApplicationContext()的时候参数传的是什么了，分别执行两个方法。\nloadBeanDefinitions\n需要注意的是，执行的方法虽然还是loadBeanDefinitions 但是已经执行的不是原来那个对象的方法了，此时到了AbstractBeanDefinitionReader对象。\n\n@Overridepublic int loadBeanDefinitions(String... locations) throws BeanDefinitionStoreException &#123;    Assert.notNull(locations, &quot;Location array must not be null&quot;);    int count = 0;    for (String location : locations) &#123;        count += loadBeanDefinitions(location);    &#125;    return count;&#125;\n\n因为参数是一个字符串的数组，此处对数组进行遍历，对字符串的资源地址再次执行loadBeanDefinitions方法。\n返回结果是一个int，也就是对应的资源地址加载成功的Bean数量。\nloadBeanDefinitions\n随后经历数次loadBeanDefinitions方法，直接跳过了…\n\ndoLoadBeanDefinitions\nSpring源码中很多名字叫 doXXXX 的对象才是真正干活的对象。\n\ntry &#123;    // 此处获取xml文件的document对象，这个解析过程是由documentLoader完成的,    // 从String[] -string-Resource[]- resource,最终开始将resource读取成一个document文档    // 根据文档的节点信息封装成一个个的BeanDefinition对象    Document doc = doLoadDocument(inputSource, resource);    int count = registerBeanDefinitions(doc, resource);    if (logger.isDebugEnabled()) &#123;        logger.debug(&quot;Loaded &quot; + count + &quot; bean definitions from &quot; + resource);    &#125;    return count;&#125;\n\n\n\ndoRegisterBeanDefinitionsprotected void doRegisterBeanDefinitions(Element root) &#123;    BeanDefinitionParserDelegate parent = this.delegate;    this.delegate = createDelegate(getReaderContext(), root, parent);    if (this.delegate.isDefaultNamespace(root)) &#123;        String profileSpec = root.getAttribute(PROFILE_ATTRIBUTE);        if (StringUtils.hasText(profileSpec)) &#123;            String[] specifiedProfiles = StringUtils.tokenizeToStringArray(                profileSpec, BeanDefinitionParserDelegate.MULTI_VALUE_ATTRIBUTE_DELIMITERS);                        if (!getReaderContext().getEnvironment().acceptsProfiles(specifiedProfiles)) &#123;                if (logger.isDebugEnabled()) &#123;                    logger.debug(&quot;Skipped XML bean definition file due to specified profiles [&quot; + profileSpec +                                 &quot;] not matching: &quot; + getReaderContext().getResource());                &#125;                return;            &#125;        &#125;    &#125;        preProcessXml(root);    // 解析BeanDefinitions    parseBeanDefinitions(root, this.delegate);    postProcessXml(root);    this.delegate = parent;&#125;\n\nparseBeanDefinitionsprotected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) &#123;    if (delegate.isDefaultNamespace(root)) &#123;        NodeList nl = root.getChildNodes();        for (int i = 0; i &lt; nl.getLength(); i++) &#123;            Node node = nl.item(i);            if (node instanceof Element) &#123;                Element ele = (Element) node;                if (delegate.isDefaultNamespace(ele)) &#123;                    parseDefaultElement(ele, delegate);                &#125;                else &#123;                    delegate.parseCustomElement(ele);                &#125;            &#125;        &#125;    &#125;    else &#123;        delegate.parseCustomElement(root);    &#125;&#125;\n\n对整个XML文件进行逐行变量\nparseDefaultElementprivate void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) &#123;    if (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) &#123;        importBeanDefinitionResource(ele);    &#125;    else if (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) &#123;        processAliasRegistration(ele);    &#125;    else if (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) &#123;        processBeanDefinition(ele, delegate);    &#125;    else if (delegate.nodeNameEquals(ele, NESTED_BEANS_ELEMENT)) &#123;        // recurse        doRegisterBeanDefinitions(ele);    &#125;&#125;\n\n根据标签的类型进行解析，&lt;bean&gt;&lt;/bean&gt;自然会执行processBeanDefinition\nprocessBeanDefinition/** * Process the given bean element, parsing the bean definition * and registering it with the registry. */protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) &#123;    // beanDefinitionHolder是beanDefinition对象的封装类，封装了BeanDefinition，bean的名字和别名，用它来完成向IOC容器的注册    // 得到这个对象就意味着beandefinition是通过BeanDefinitionParserDelegate    // 对xml元素的信息按照spring的bean规则进行解析得到的    BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele);    if (bdHolder != null) &#123;        bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder);        try &#123;            // Register the final decorated instance.            // 向ioc容器注册解析得到的beandefinition的地方            BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, getReaderContext().getRegistry());        &#125;        catch (BeanDefinitionStoreException ex) &#123;            getReaderContext().error(&quot;Failed to register bean definition with name &#x27;&quot; +                                     bdHolder.getBeanName() + &quot;&#x27;&quot;, ele, ex);        &#125;        // Send registration event.        // 在beandefinition向ioc容器注册完成之后，发送消息        getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder));    &#125;&#125;\n\nregisterBeanDefinitionpublic static void registerBeanDefinition(      BeanDefinitionHolder definitionHolder, BeanDefinitionRegistry registry)      throws BeanDefinitionStoreException &#123;   // 获取BeanName   String beanName = definitionHolder.getBeanName();   // 使用beanName做唯一标识注册   registry.registerBeanDefinition(beanName, definitionHolder.getBeanDefinition());   // Register aliases for bean name, if any.   // 注册所有的别名   String[] aliases = definitionHolder.getAliases();   if (aliases != null) &#123;      for (String alias : aliases) &#123;         registry.registerAlias(beanName, alias);      &#125;   &#125;&#125;\n\n\n\nregisterBeanDefinition@Overridepublic void registerBeanDefinition(String beanName, BeanDefinition beanDefinition)    throws BeanDefinitionStoreException &#123;    // 省略 ....    else &#123;        // Still in startup registration phase        // 注册beanDefinition        this.beanDefinitionMap.put(beanName, beanDefinition);        // 记录beanName        this.beanDefinitionNames.add(beanName);        removeManualSingletonName(beanName);    &#125;    this.frozenBeanDefinitionNames = null;    // 省略 ....&#125;\n\n此时registerBeanDefinition已经回到了DefaultListableBeanFactory方法，正常执行来说主要会做如下操作：\n\n将解析成功的Bean对象名称加到beanDefinitionNames的列表中\n将BeanName和BeanDefinition的键值对存放在beanDefinitionMap中\n\n至此，就将五花八门的自定义Bean解析为了BeanDefinition，并存放在了BeanFactory中，后续需要加载某个Bean时只需要根据名称到map中找到对应的BeanDefinition即可，然后执行doCreateBean方法。\n","categories":["Java"],"tags":["Spring","Spring源码解析系列"]},{"title":"Spring源码解析(三) 注册Bean处理器对象","url":"/2022/01/21/Spring%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E4%B8%89-%E6%B3%A8%E5%86%8CBean%E5%A4%84%E7%90%86%E5%99%A8/","content":"前言前面讲了 BeanFactoryPostProcessor本文讲 BeanPostProcessor，两种总给人一种很像的感觉，首先我们先总结一下两者的区别（可能不全，逐步补充）。\n\n\n\n\nBeanFactoryPostProcessor\nBeanPostProcessor\n\n\n\n角色\nSpring IOC 所管理的一个Bean\nSpring IOC 所管理的一个Bean\n\n\n执行时间\nIOC容器启动的时候，准确来说是 refresh时\n每次 getBean时都会调用\n\n\n执行次数\n有且只有一次\n无数次\n\n\n实现方式\n实现 BeanFactoryPostProcessor接口\n实现 BeanPostProcessor接口\n\n\n排序\n可排序\n可排序\n\n\n获取到的参数\nConfigurableListableBeanFactory 也就是Spring 的核心BeanFactory\nObject,执行 getBean的对象\n\n\n操作性\nBean工厂的操作他都能执行，例如：可以获取 BeanDefinition并修改；提前注入Bean对象（执行该步骤时，自定义的Bean都没有注入）；等等等等\n修改当前Bean的信息\n\n\n本文主要介绍了 refresh 方法中的 registerBeanPostProcessors(beanFactory); , 用于注册Bean的处理器对象。\n⚠️ 只是注册，不执行。\n抽象流程其实和 BeanFactoryPostProcessor的调用流程类似。\n\n根据类型查询所有 BeanPostProcessor的实现对象，得到一个数组\n记录 BeanPostProcessor的数量（会有一个+1的操作，因为要算上下面手动注册的 BeanPostProcessorChecker）\n手动注册 BeanPostProcessorChecker\n准备一些容器，用于存放收集来的各种处理器\n遍历 BeanPostProcessor的实现对象数组\n如果 BeanPostProcessor实现了 PriorityOrdered接口，获取 Bean对象并添加到 priorityOrderedPostProcessors列表（和后面的有所不同，这里添加到列表的是 BeanPostProcessor具体 Bean对象,其他都是字符串）\n如果获取到的 Bean对象还实现了 MergedBeanDefinitionPostProcessor则添加到 internalPostProcessors列表\n\n\n如果没有实现 PriorityOrdered接口，但是实现了 Order接口，添加到 orderedPostProcessorNames列表中\n如果没有实现排序相关的接口，普通对象添加到 nonOrderedPostProcessorNames列表中\n\n\n对 priorityOrderedPostProcessors 进行排序，并注册\n遍历 orderedPostProcessorNames列表\n根据 ppName找到对应的 BeanPostProcessor实例对象\n添加到 orderedPostProcessors集合中\n如果实现了 MergedBeanDefinitionPostProcessor接口，那么则将 ppName对应的 bean实例添加到 internalPostProcessors中\n\n\n对 orderedPostProcessors进行排序\n注册实现了 Ordered接口的 BeanPostProcessor实例添加到 BeanFactory中\n遍历 nonOrderedPostProcessorNames 列表\n根据 ppName找到对应的 BeanPostProcessor实例对象\n添加到 nonOrderedPostProcessors列表\n如果实现了 MergedBeanDefinitionPostProcessor 则添加到 internalPostProcessors列表\n遍历 nonOrderedPostProcessors列表进行注册\n对 internalPostProcessors 列表进行排序，并遍历注册\n最后注册 ApplicationListenerDetector到 BeanFactory中\n\n总结整个流程类似于调用 BeanFactoryPostProcessor，通过类型获取所有的Bean处理器对象。然后根据顺序进行注册。\n顺序为：\n\n实现了 PriorityOrdered接口的处理器对象\n实现了 Ordered接口的处理器对象\n没有实现排序的处理器对象\n实现了 MergedBeanDefinitionPostProcessor接口的处理器对象\n\n","categories":["Java"],"tags":["Spring","Spring源码解析系列"]},{"title":"Spring源码解析(二) 调用Bean工厂处理器对象","url":"/2022/01/21/Spring%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E4%BA%8C-%E8%B0%83%E7%94%A8BeanFactory%E5%A4%84%E7%90%86%E5%99%A8/","content":"前言BeanFactoryPostProcessor 是 Spring 中很重要的一种机制，本文主要介绍了 BeanFactoryPostProcessor 的注册与执行。\n该步骤相比较创建 BeanFactory并注册 BeanDefinition要简单清晰的多，没有过多的调核心方法 PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(beanFactory, getBeanFactoryPostProcessors());只有两百多行。\n基本概念\nBeanFactoryPostProcessor\nBeanFactory在创建完成后的后置处理器，可以自定义一些业务代码，可用于：修改BeanDefinition，提前CreateBean。其实和 BeanPostProcessor相似。\n\nBeanDefinitionRegistryPostProcessor\n他是 BeanFactoryPostProcessor的子接口，相比较他的父接口多了一个执行方法 postProcessBeanDefinitionRegistry\n优先级要大于 BeanFactoryPostProcessor。\n\nPriorityOrdered 和 Ordered\n其实都是用于定义优先级的。\n\n\n\n优先级 在本方法中非常重要，看似简单的代码被拉长到了两百多行就是因为优先级导致的。\n上面概念中多次提到优先级这个词，下面用简单暴力的话解答一下。\n\nBeanDefinitionRegistryPostProcessor 的优先级一定大于 BeanFactoryPostProcessor ，就算天塌了也是先执行 BeanDefinitionRegistryPostProcessor。\n被定义 PriorityOrdered 的 Bean 优先级一定大于 Ordered 的对象。\nN个对象如果同样被 PriorityOrdered 或者 Ordered 定义，则比较其值大小。\n\n\n抽象流程\n\n\n图片比较大，建议放大后对比源码观看。\n\n如果用简单的话总结执行步骤：\n\n第一步：执行预设的 BeanDefinitionRegistryPostProcessor 对象（大部分情况下没有预设对象）\n第二步：执行现了PriorityOrdered接口的BeanDefinitionRegistryPostProcessor\n第三步：执行实现了Ordered接口的BeanDefinitionRegistryPostProcessor\n第四步：执行剩余的BeanDefinitionRegistryPostProcessor\n第五步：查询实现 BeanFactoryPostProcessor 接口的对象，根据是否已执行、是否实现PriorityOrdered或Ordered接口去分类，分别加入不同的列表，用于最后统一的执行\n第六步：统一遍历执行前面收集到的处理器对象\n第七步：执行结束，清理元数据缓存\n\n如何预设的处理器对象前面有说过，在 invokeBeanFactoryPostProcessors中将 getBeanFactoryPostProcessors()作为参数传递给了 invokeBeanFactoryPostProcessors。\n只需要在执行此步骤之前将前置处理器添加到 BeanFactoryPostProcessors中即可（⚠️ 此处的 BeanFactoryPostProcessors有一个 s，是一个列表）。\n例如：\n\npostProcessBeanFactory是一个预留的方法，里面没有实现，我们只需要重写该方法即可\nprotected void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) &#123;  this.addBeanFactoryPostProcessor(new PreBeanFactoryPostProcessor());&#125;\n\n其中 PreBeanFactoryPostProcessor是自己实现的一个处理器对象。\n\n当然也可以直接修改源码在任何地方执行 this.addBeanFactoryPostProcessor\n\n\n源码\n其实整个流程走下来，发现这块源码没啥难的就不详细介绍了，结合流程图和官方注释问题不大。\n\n总结一句话概括：就一个执行BeanFactory处理器对象的方法，只不过因为优先级的问题搞得有点复杂。\n","categories":["Java"],"tags":["Spring","Spring源码解析系列"]},{"title":"Spring源码解析(五) 解决循环依赖问题","url":"/2022/01/25/Spring%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E4%BA%94-%E8%A7%A3%E5%86%B3%E5%BE%AA%E7%8E%AF%E4%BE%9D%E8%B5%96%E9%97%AE%E9%A2%98/","content":"前言什么是依赖注入?@Componentpublic class A &#123;    private B b;&#125;@Componentpublic class B &#123;    private A a;&#125;\n\n\n如上代码，A和B相互依赖对方\n\n\n注入A对象的时候发现需要B对象，那就先去注入B对象\n注入B对象的时候发现需要A对象，那就先去注入A对象\n注入A对象的时候发现需要B对象，那就先去注入B对象\n注入B对象的时候发现需要A对象，那就先去注入A对象\n\n… 不断循环导致注入失败。\n解决方案\n一句话回答如何解决循环依赖：Spring解决循环依赖的核心思想在于提前曝光\n创建Bean的时候将流程分为两步走，先实例化再初始化只实例化没有初始化的叫半成品（放置在二级缓存**earlySingletonObjects）反之则是完成品（放置在一级缓存singletonObjects**）。\nA注入对象时遇到依赖其他属性先到完成品池子寻找对象，没有则去半成品池子，半成品也没有则创建一个半成品。A对象注入时先不考虑属性完成与否，先将属性逐个注入，保证自己是个完成品并放置到一级缓存，然后B对象依赖A属性时直接从一级缓存取出即可。\n\n这个机制的一个前提：无论怎么填充属性，对象的内存地址是不会改变的。\n","categories":["Java"],"tags":["Spring","Spring源码解析系列"]},{"title":"Spring源码解析(四) 初始化Bean工厂","url":"/2022/01/21/Spring%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E5%9B%9B-%E5%88%9D%E5%A7%8B%E5%8C%96Bean%E5%B7%A5%E5%8E%82/","content":"前言初始化Bean工厂 这个名字起的好像有点大，在前面创建了Bean工厂后填充了一些属性，创建了BeanDifinition但是一直没有注入Bean。\n本章Bean就将正式入住了。\n\n建议结合 “Bean的生命周期” 这个问题一起思考\n\n抽象流程源码流程\n\nBean 的生命周期\n源码BeanDefinition 的合并机制\n在创建工厂并读取Bean配置时会创建一个BeanDefinition对象，BeanDefinition是一个高级接口，准确来说时创建为了GenericBeanDefinition。\n在经历合并后会变成RootBeanDefinition。\n虽然执行了getMergedLocalBeanDefinition方法，但是正常来说这不是真正的第一次合并。\n\n\nBeanDefinition 什么时候合并的？    \n在调用BeanFactoryPostProcessor时就已经执行了。\nString[] postProcessorNames =                    beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false);\n\n在根据类型获取postProcessorNames时，遍历所有的BeanDefinitionName获取合并后的BeanDefinition\nRootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName);\n为什么要这么做？\n\nGenericBeanDefinition相比较Root多了一个ParentName的属性，用于记录Bean的parent参数。\n通过GenericBeanDefinition先定义好对象的父子关系，后面的RootBeanDefinition根据前面所定义的父子关系去创建Bean，没有父类直接创建，存在父类递归的去创建父类（父类也可能存在父类）。\n如果不提前创建父子关系，很有可能出现创建子类时发现父类还没有创建此时就需要大量的判断和查询显然是没有两阶段机制优雅\n\n\n\nFactoryBean 处理机制\nFactoryBean 和 BeanFactory 看的很像，提供的服务也很像，但是底层时完全不同的。在 Spring 容器需要针对的做出一套处理机制/\n\n// 判断是否为FactoryBeanif (isFactoryBean(beanName)) &#123;    // [1] 根据（&amp;+beanName）的规则来获取对象     Object bean = getBean(FACTORY_BEAN_PREFIX + beanName);    // 进行类型转换    if (bean instanceof FactoryBean) &#123;        FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) bean;        // 判断这个FactoryBean是否希望立即初始化        boolean isEagerInit;        if (System.getSecurityManager() != null &amp;&amp; factory instanceof SmartFactoryBean) &#123;            isEagerInit = AccessController.doPrivileged(                (PrivilegedAction&lt;Boolean&gt;) ((SmartFactoryBean&lt;?&gt;) factory)::isEagerInit,                getAccessControlContext());        &#125;        else &#123;            // 只有实现SmartFactoryBean才可以定义是否急迫            isEagerInit = (factory instanceof SmartFactoryBean &amp;&amp;                           ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit());        &#125;        // [2] 如果希望急切的初始化，则通过beanName获取bean实例        if (isEagerInit) &#123;            getBean(beanName);        &#125;    &#125;&#125;else &#123;    // [3] 如果beanName对应的bean不是FactoryBean，只是普通的bean，通过beanName获取bean实例    getBean(beanName);&#125;\n\n\n[1]  根据（&amp;+beanName）的规则来获取对象\n\nFactoryBean 是一个特殊的Bean！\nFactoryBean 是一个特殊的Bean！\nFactoryBean 是一个特殊的Bean！\n⚠️ 重要的事情说三遍，既然也是Bean也是需要交予Spring容器管理的。正是因为FactoryBean的特殊性，拥有了与其他Bean不同BeanName规则（需要前缀加一个&amp;）\n\n\n[2] 急切的初始化\n\n正常流程来说，FactoryBean 的对象在初始阶段是不会被注入到Spring容器的，需要在getBean()才会注入。\n如果isEagerInit = true 就会立刻注入。\n⚠️ 此时注入的是实际的Bean，不是上面所注入的FactoryBean\n\n\n[3] 普通的Bean\n\n不是FactoryBean正常执行getBean()即可\n\n# doGetBean// 前面说了 FactoryBean 在BeanName起名有独特的规范// 该方法就是去掉了&amp;的前缀String beanName = transformedBeanName(name);Object bean;// 提前检查单例缓存中是否有手动注册的单例对象，跟循环依赖有关联Object sharedInstance = getSingleton(beanName);// 如果bean的单例对象找到了，且没有创建bean实例时要使用的参数if (sharedInstance != null &amp;&amp; args == null) &#123;    // ...省略部分源码        // 返回对象的实例    bean = getObjectForBeanInstance(sharedInstance, name, beanName, null);&#125;\n\nprotected Object getObjectForBeanInstance(    Object beanInstance, String name, String beanName, @Nullable RootBeanDefinition mbd) &#123;    // name 如果是&amp;作为前缀    if (BeanFactoryUtils.isFactoryDereference(name)) &#123;        // 如果beanInstance是NullBean实例        if (beanInstance instanceof NullBean) &#123;            // 返回beanInstance            return beanInstance;        &#125;        // 如果beanInstance不是FactoryBean实例        if (!(beanInstance instanceof FactoryBean)) &#123;            // 抛出Bean不是一个Factory异常            throw new BeanIsNotAFactoryException(beanName, beanInstance.getClass());        &#125;        // 如果mbd不为null        if (mbd != null) &#123;            // 设置mbd是否是FactoryBean标记为true            mbd.isFactoryBean = true;        &#125;        // 返回beanInstance        return beanInstance;    &#125;    // 如果不是FactoryBean类型直接返回    if (!(beanInstance instanceof FactoryBean)) &#123;        return beanInstance;    &#125;    // 定义为bean公开的对象，初始化为null    Object object = null;    // 如果mbd不为null    if (mbd != null) &#123;        // 更新mbd的是否是FactoryBean标记为true        mbd.isFactoryBean = true;    &#125;    else &#123;        // 从FactoryBean获得的对象缓存集中获取beanName对应的Bean对象        object = getCachedObjectForFactoryBean(beanName);    &#125;    // 如果object为null    if (object == null) &#123;        // Return bean instance from factory.        // 从工厂返回Bean实例        // 将beanInstance强转为FactoryBean对象        FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) beanInstance;        // Caches object obtained from FactoryBean if it is a singleton.        // 如果是单例对象，则缓存从FactoryBean获得的对象、        // 如果mbd为null&amp;&amp;该BeanFactory包含beanName的BeanDefinition对象。        if (mbd == null &amp;&amp; containsBeanDefinition(beanName)) &#123;            //获取beanName合并后的本地RootBeanDefintiond对象            mbd = getMergedLocalBeanDefinition(beanName);        &#125;        // 是否是&#x27;synthetic&#x27;标记：mbd不为null &amp;&amp; 返回此bean定义是否是&quot;synthetic&quot;【一般是指只有AOP相关的prointCut配置或者        // Advice配置才会将 synthetic设置为true】        boolean synthetic = (mbd != null &amp;&amp; mbd.isSynthetic());        // 从BeanFactory对象中获取管理的对象.如果不是synthetic会对其对象进行该工厂的后置处理        object = getObjectFromFactoryBean(factory, beanName, !synthetic);    &#125;    // 返回为bean公开的对象    return object;&#125;\n\ngetObjectForBeanInstance 方法可能出现三种情况：\n\n普通的Bean对象：直接返回Instance\n带有&amp;符的FactoryBean：返回工厂Bean\n不带&amp;符的FactoryBean：返回对应工厂的实际Bean\n\n\n\nBean 是否正在创建中# doGetBeanif (isPrototypeCurrentlyInCreation(beanName)) &#123;    throw new BeanCurrentlyInCreationException(beanName);&#125;protected boolean isPrototypeCurrentlyInCreation(String beanName) &#123;    // 获取当前正在创建的bean名称【线程本地】    Object curVal = this.prototypesCurrentlyInCreation.get();    return (curVal != null &amp;&amp;            (curVal.equals(beanName) || (curVal instanceof Set &amp;&amp; ((Set&lt;?&gt;) curVal).contains(beanName))));&#125;\n\n\n❗返回True表示当前Bean正在创建\n如果当前创建的Bean不为空且等于当前BeanName  或  当前正在创建的bean名称是Set集合，并包含该beanName\n\n父工厂问题通过BeanName获取Object时，如果存在父工厂且当前工厂不存在该BeanName时可能就嘀咕，它是不是存在父工厂了？\n解决方案就是：通过递归的方式层层查找。\nBean存在依赖问题例如在Xml定义Bean时可能设定一个 parent 属性：\n&lt;bean id=&quot;student&quot; class=&quot;com.lksun.debug.entity.Student&quot;&gt;    &lt;property name=&quot;name&quot; value=&quot;miaomiao&quot;/&gt;    &lt;property name=&quot;age&quot; value=&quot;8&quot;/&gt;&lt;/bean&gt;    &lt;bean id=&quot;monitor&quot; class=&quot;com.lksun.debug.entity.Monitor&quot; parent=&quot;student&quot; &gt;    &lt;property name=&quot;className&quot; value=&quot;3-2&quot;/&gt;&lt;/bean&gt;\n\nstudent作为monitor的父类或者依赖对象，先有鸡还是先有蛋就很重要了。\n必须先创建了依赖Bean再创建当前对象。\n解决方法同样使用递归，获取当前对象是否有依赖对象，然后层层判断\n解析Bean ClassClass&lt;?&gt; resolvedClass = resolveBeanClass(mbd, beanName);\n\n获取到了Class对象后只需要一步resolvedClass.newInstance();就可以获取到Object，但是Spring没有直接获取。\n验证及准备覆盖的方法\ntodo ….\nlookup-method  replace-method\n\n实际创建bean的方法 doCreateBean()根据不同的策略创建BeaninstanceWrapper = createBeanInstance(beanName, mbd, args);\n\n获取真实Bean对象Object bean = instanceWrapper.getWrappedInstance();\n\n加入缓存\n关于解决循环依赖相关的问题后续着重讲\n\naddSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, bean));protected void addSingletonFactory(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123;    Assert.notNull(singletonFactory, &quot;Singleton factory must not be null&quot;);        if (!this.singletonObjects.containsKey(beanName)) &#123;            this.singletonFactories.put(beanName, singletonFactory);            this.earlySingletonObjects.remove(beanName);            this.registeredSingletons.add(beanName);        &#125;    &#125;&#125;\n\n\n判断一级缓存不存在当前Bean\n然后执行：\n1.将beanName,singletonFactory放到单例工厂的缓存(三级缓存)\n2.在二级缓存删除当前Bean\n3.将beanName添加已注册的单例集中\n\n填充populateBean(beanName, mbd, instanceWrapper);\n\n\n\n执行初始化逻辑exposedObject = initializeBean(beanName, exposedObject, mbd);\n\n\n做了如下操作：\n1.执行Aware接口处理器\n2.BeanPostProcessor前置方法\n3.调用初始化方法，先调用bean的InitializingBean接口方法，后调用bean的自定义初始化方法\n4.BeanPostProcessor前置方法后置方法\n\n注册Disposable// 注册bean对象，方便后续在容器销毁的时候销毁对象registerDisposableBeanIfNecessary(beanName, bean, mbd);\n\n标记为“未创建”// 创建单例后的回调,默认实现将单例标记为不在创建中afterSingletonCreation(beanName);\n\n添加到一级缓存\n等一切都完成后添加到一级缓存\n\n# getSingleton()addSingleton(beanName, singletonObject);protected void addSingleton(String beanName, Object singletonObject) &#123;    synchronized (this.singletonObjects) &#123;        // 将映射关系添加到单例对象的高速缓存中        this.singletonObjects.put(beanName, singletonObject);        // 移除beanName在单例工厂缓存中的数据        this.singletonFactories.remove(beanName);        // 移除beanName在早期单例对象的高速缓存的数据        this.earlySingletonObjects.remove(beanName);        // 将beanName添加到已注册的单例集中        this.registeredSingletons.add(beanName);    &#125;&#125;\n\n\n1.先添加到一级缓存\n2.在三级缓存移除\n3.在二级缓存移除\n4.保存到单例集合中\n\n","categories":["Java"],"tags":["Spring","Spring源码解析系列"]},{"title":"Hello World","url":"/2022/01/06/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server$ hexo server\n\nMore info: Server\nGenerate static files$ hexo generate\n\nMore info: Generating\nDeploy to remote sites$ hexo deploy\n\nMore info: Deployment\n"},{"title":"String str = \"hello\"; 和 String = new String(\"hello\"); 的区别","url":"/2022/01/08/hello-%E5%92%8C-String-new-String-hello-%E7%9A%84%E5%8C%BA%E5%88%AB/","content":"前言String str = &quot;hello&quot;; String = new String(&quot;hello&quot;); \n\n如上两个方法都可以得到我们想要的一个字符串，但是区别还是有的.\n堆和常量池的关系\n堆（Heap）\n\nJava堆（Java Heap）是Java虚拟机所管理的内存中最大的一块。Heap是被所有线程共享的一块内存区域，在虚拟机启动时被创建。Heap的唯一目的就是存放对象实例，几乎所有的对象实例都是在这里分配内存。\n\n字符串常量池\n\n在jdk1.7之前的版本中，字符串常量池是存在于永久代中，在永久代和堆中的地址是完全分离的，不会直接引用。在jdk1.7+之后的版本中，字符串常量池被放在了堆中。\n声明一个字符串String Str1 = “hello”;内存分布对于str1来说，由于是常量赋值，str1直接指向字符串常量池中的数据，并且只有一份。\nString str2 = new String(“hello”);内存分布String str2 = new String(&quot;ab&quot;);在执行过程中，JVM首先在字符串常量池中查看字符串对象“ab”是否存在，若不存在，则现在字符串常量池中创建“ab”对象，然后再Heap中创建一个新的“ab”字符串对象。若字符串常量池中已经存在了字符串对象“ab”，则直接在堆中创建一个字符串对象“ab”，不许在字符串常量池中创建对象，但是不管是那种方式，栈中的str2始终指向的是Heap中的字符串对象。若字符串常量池中原本没有“ab”对象，String str2 = new String(“ab”);执行后会在字符串常量池和堆中各创建一个对象，即创建两个对象，若字符串常量池中已存在了“ab”对象，则只会在Heap中创建，即只创建了一个对象。\n示例示例一String str1 = &quot;rush b&quot;;String str2 = &quot;rush b&quot;;System.out.println(str1 == str2); // return trueSystem.out.println(str1.equals(str2)); // return trueSystem.out.println(System.identityHashCode(str1)); // return 1163157884System.out.println(System.identityHashCode(str2)); // return 1163157884\n\nstr1 和 str2 是完全不同的两个变量，在栈空间中地址是不同的，但是通过 System.identityHashCode发现他们其实也相同。如下图所示。\n示例二public class StringDemo &#123;\tpublic static void main(String[] args) &#123;\t\tString str1 = &quot;ab&quot;;\t\tString str2 = str1.intern();\t\tSystem.out.println(str1 == str2); // true\t&#125;&#125;\n\n\n\n代码非常简单，str1就是最普通的常量赋值，会直接在字符串常量池中创建出”ab”对象 str1调用intern方法得到返回值赋值给str2，intern方法会现在字符串常量池中检测是否已经存在”ab”字符串对象，若已经存在，直接把字符串常量池里”ab”对象的地址赋值给str2，所以str1==str2为true。\n\n致谢作者：saojiatete链接：https://juejin.cn/post/6844904015830974472来源：掘金著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n","categories":["Java"],"tags":["JVM","造火箭"]},{"title":"关于Mysql的江湖传言","url":"/2022/01/08/%E5%85%B3%E4%BA%8EMysql%E7%9A%84%E6%B1%9F%E6%B9%96%E4%BC%A0%E8%A8%80/","content":"谣言粉碎机谁说必须遵循最左匹配原则?创建组合索引在查询语句时必须遵循最左匹配原则，即 where 时组合索引的第一个索引列必须在最前面。不然会索引失效!这是常见的八股文了，原理上没毛病，在B+树上每个非叶子节点不可能逐个比较条件语句，通过组合索引的第一个匹配。但是优化器在执行前会帮助我们调整where 的顺序，开发者不强制顺序，但是最好还是遵循一下。\n\nSELECT * FROM client  WHERE age = 18 AND shop_id = 1SELECT * FROM client  WHERE shop_id = 1 AND age = 18\n\n如上两条SQL，结果一样只是where的顺序不同，组合索引字段设置为 &quot;shop_id &quot;,&quot;age &quot;，如果按照江湖传言最左匹配的话第一条时无法命中索引的，但是通过 EXPLAIN查看，两条语句的结果相同。\n谁说组合索引遇到范围查询就失效？在一个组合索引中如：where shop_id = 2 and client_type = 2 and age &gt;18 ,即使 &quot;shop_id &quot;,&quot;age &quot;,&quot;client_type &quot;有一个组合索引但是因为 age的范围查询会不走索引。\n首先这是错误的，在MySQL5.6版本后推出了索引下推机制，非等值查询也并不会导致索引失效了。\n谁说使用SQL函数会索引失效?查询中不能使用内置的函数否则会索引失效，也是常见的八股文之一了那么事实情况是什么样？\nEXPLAIN SELECT * FROM client WHERE shop_id = 1 AND nick_name = CONCAT(&quot;雷&quot;,&quot;达&quot;) \n\n在如上语句中使用了 CONCAT但是查询计划还是命中索引\n还例如：\nEXPLAIN SELECT * FROM client WHERE shop_id = 1 AND create_time = NOW()\n\n同样使用了索引\n再或者\nEXPLAIN SELECT max(age) FROM client WHERE shop_id = 1 AND gmt_create = NOW()\n\n\n\n5.7新增特性:可以使用函数索引\n\n谁说计算会导致索引失效?还是前面的例子\nEXPLAIN SELECT max(age) FROM client WHERE shop_id = 10-9 AND gmt_create = NOW()\n\nshop_id没有直接写1 ,使用了 10-9，最终的效果时一样的。\n","categories":["MySQL"],"tags":["造火箭","谎言粉碎机","MySQL调优"]}]